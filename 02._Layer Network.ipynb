{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이름/학번\n",
    "\n",
    "이름: 이예주\n",
    "\n",
    "학번: 20193237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example dataset\n",
    "\n",
    "강의를 위해서 임의의 dataset을 준비하겠습니다.\n",
    "예제로 봐주시고, 큰 물리적 의미는 부여하지 않겠습니다.\n",
    "\n",
    "- Data는 장미과와 국화과의 A 효소, B 효소, C 효소, D 효소를 측정한 값이라고 가정합니다.\n",
    "- Label은 각 sample이 장미인지 (0) 국화인지 (1)에 대한 정보라고 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10  # 행의 수(m) - 입력 데이터의 샘플 수\n",
    "num_feature = 4  # 열의 수 / 벡터의 길이(n)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "X_batch = torch.randn(batch_size, num_feature)  \n",
    "Y_batch = (torch.sum(X_batch, dim=1)>0).type(torch.float).reshape(-1,1)  # 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
       "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
       "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
       "        [ 0.1198,  1.2377,  1.1168, -0.2473],\n",
       "        [-1.3527, -1.6959,  0.5667,  0.7935],\n",
       "        [ 0.5988, -1.5551, -0.3414,  1.8530],\n",
       "        [-0.2159, -0.7425,  0.5627,  0.2596],\n",
       "        [-0.1740, -0.6787,  0.9383,  0.4889],\n",
       "        [ 1.2032,  0.0845, -1.2001, -0.0048],\n",
       "        [-0.5181, -0.3067, -1.5810,  1.7066]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 행 (1번째 꽃(x1), 2번째 꽃(x2)을 의미)\n",
    "# 열 (A효소, B효소, C효소, D효소를 의미)\n",
    "\n",
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1258, -1.1524, -0.2506, -0.4339])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch[0, :]  # 첫번째 샘플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_batch  # 각각의 레이블 ex) 첫번째는 장미(0), 4번째는 국화(1)  -> 정답값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation 정리\n",
    "\n",
    "강의자료와 비교하면 \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{X_batch} = \n",
    "\\begin{bmatrix}\n",
    "(x^{(1)})^\\top\\\\\n",
    "(x^{(2)})^\\top\\\\\n",
    "\\vdots \\\\\n",
    "(x^{(m)})^\\top\n",
    "\\end{bmatrix}, \\quad\n",
    "\\text{Y_batch} = \n",
    "\\begin{bmatrix}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "\\vdots \\\\\n",
    "y^{(m)}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Problem: Single Neuron\n",
    "\n",
    "- 한개의 neuron이 있다고 가정하고 $\\mathbb{R}^4$ 를 입력받아서 $\\mathbb{R}$로 출력한다고 가정합니다.\n",
    "- Activation 함수는 ReLU 함수, 즉, \n",
    "\\begin{align*}\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "\\end{align*}\n",
    "를 사용한다고 가정합니다.\n",
    "\n",
    "Neuron을 통해서 batch sample 전체를\n",
    "\\begin{align*}\n",
    "Z = \\begin{bmatrix}\n",
    "(w^T x^{(1)} + b)^T \\\\\n",
    "(w^T x^{(2)} + b)^T \\\\\n",
    "\\vdots \\\\\n",
    "(w^T x^{(m)} + b)^T\n",
    "\\end{bmatrix} \n",
    "\\end{align*}\n",
    "연산을 수행해서 $Z$를 구하세요.\n",
    "\n",
    "- $w$는 random Gaussian으로 생성하세요. 위에서 예기한 입력과 출력이 맞도록 weight를 생성하세요.\n",
    "- Bias $b$는 1로 설정\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**메모)**   \n",
    "w와x의 내적 + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0209],\n",
       "        [-0.7185],\n",
       "        [ 0.5186],\n",
       "        [-1.3125]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1920],\n",
       "        [ 0.5428],\n",
       "        [-2.2188],\n",
       "        [ 0.2590]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(num_feature, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.0662],\n",
      "        [ 1.6351],\n",
      "        [ 1.1136],\n",
      "        [ 0.7891],\n",
      "        [ 2.6685],\n",
      "        [-0.6325],\n",
      "        [ 1.5096],\n",
      "        [ 1.3267],\n",
      "        [-0.6050],\n",
      "        [-0.3120]])\n"
     ]
    }
   ],
   "source": [
    "# 답 작성\n",
    "\n",
    "# torch.randn은 가우시안으로 생성하는 것이다.\n",
    "\n",
    "W = torch.randn(num_feature, 1) # torch.randn(입력 뉴런수(크기), 출력 뉴런수(크기))\n",
    "b = 1  # bias -> 출력크기가 1이기 때문에 bias도 1\n",
    "Z = torch.matmul(X_batch, W)+b  # 이전 layer의 입력 x W -> 여기에서는 single 뉴런이기 때문에 이전 layer의 입력이 X인 것이다.\n",
    "print(Z)\n",
    "\n",
    "# 출력 결과 해석\n",
    "# 위에서부터 '첫번째 샘플에 대한 뉴런 출력, 두번째 샘플에 대한 뉴런 출력, ....,'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원하는 연산을 하는지 확인하도록 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.0662],\n",
      "        [ 1.6351],\n",
      "        [ 1.1136],\n",
      "        [ 0.7891],\n",
      "        [ 2.6685],\n",
      "        [-0.6325],\n",
      "        [ 1.5096],\n",
      "        [ 1.3267],\n",
      "        [-0.6050],\n",
      "        [-0.3120]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\이예주\\AppData\\Local\\Temp\\ipykernel_14172\\1446079948.py:3: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3281.)\n",
      "  z_loop[i,:] = torch.matmul(W.T, X_batch[i,:].T)+b\n"
     ]
    }
   ],
   "source": [
    "z_loop = torch.empty(batch_size, 1)\n",
    "for i in torch.arange(batch_size):\n",
    "    z_loop[i,:] = torch.matmul(W.T, X_batch[i,:].T)+b\n",
    "print(z_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "ReLU 함수를 작성하고 위에서 찾은 $Z$의 각 원소에 ReLU 함수를 적용하여 `a`라는 변수에 저장하세요.\n",
    "\n",
    "- torch.clamp() 함수를 공부하고 적용하세요\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.clamp()는 min 혹은 max의 범주에 해당하도록 값을 변경하는 것을 의미한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.0662],\n",
      "        [1.6351],\n",
      "        [1.1136],\n",
      "        [0.7891],\n",
      "        [2.6685],\n",
      "        [0.0000],\n",
      "        [1.5096],\n",
      "        [1.3267],\n",
      "        [0.0000],\n",
      "        [0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 답 작성:\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.clamp(x, min=0)  # min보다 작은 값들은 다 0으로 바꿔준다는 뜻.\n",
    "\n",
    "A = ReLU(Z)  # ReLU에 Z를 씌웠을 때, 음수(-)는 다 0으로 바뀐다.\n",
    "\n",
    "print(A)\n",
    "\n",
    "# 복습 -------------\n",
    "# ReLU 함수는 각 원소마다 입력이 0보다 크면 그 값을 그대로 출력하고, 0보다 작으면 0을 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Network\n",
    "\n",
    "뉴런이 여러개이지만 layer가 한개짜리. layer가 한개니까 입력은 당연히 X라고 봄.  \n",
    "입력은 10 x 4   \n",
    "출력은 5짜리(k=5)\n",
    "- 한개의 Layer에 $k=5$개의 Neuron 이 있는 network를 구성하고 출력을 구하세요\n",
    "- Activation function은 모든 neuron에 ReLU를 적용합니다\n",
    "- 모든 weight는 Gaussian 분포로 랜덤 생성하세요 `torch.randn()`\n",
    "- $i$ 번째 neuron의 weight들을 $w_i$라고 할때,\n",
    "\\begin{align*}\n",
    "\\text{W} = \\begin{bmatrix}\n",
    "w_1, w_2, w_3, w_4, w_5\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "라고 하고, weight matrix `W`를 만드세요.\n",
    "  - `W = torch.randn(???, ???)` 으로 생성  -> [차원을 맞추라는 뜻.]\n",
    "- Bias 역시 `b`라는 `tensor`에 저장하고, 각 neuron 별로 `1`로 설정합니다\n",
    "  - `b = torch.ones(???,???)`\n",
    "- 아래 problem 2-2에서 수업에서 배운 $Z$ 행렬과 $A$ 행렬을 구하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2-1\n",
    "(입력이 10x4이고 출력이 5일때,)   \n",
    "`Z` 행렬과 `A` 행렬의 차원은 어떻게 나와야하나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 답 작성)\n",
    "\n",
    "Z 는 (10,5), \n",
    "A 는 (10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2-2 \n",
    "위에서 요구한 코딩을 완성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0404,  0.2881, -0.0075, -0.9145, -1.0886]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10  # 행의 수(m)\n",
    "# num_feature = 4  # 열의 수 / 벡터의 길이(n)\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# X_batch = torch.randn(batch_size, num_feature)\n",
    "# Y_batch = (torch.sum(X_batch, dim=1)>0).type(torch.float).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0259, 0.1532, 0.7126, 0.0986, 0.0000],\n",
      "        [2.1053, 1.3770, 0.0000, 0.0000, 0.0000],\n",
      "        [4.2555, 1.3657, 1.1774, 0.0000, 1.5884],\n",
      "        [0.0000, 0.9112, 0.0000, 1.3927, 3.2091],\n",
      "        [1.6620, 0.1195, 1.6564, 0.4289, 2.8937],\n",
      "        [5.5180, 1.8361, 3.4116, 1.1107, 2.2406],\n",
      "        [1.7660, 0.8838, 0.9816, 0.3584, 2.2296],\n",
      "        [1.2537, 0.9196, 1.0118, 0.4391, 3.2824],\n",
      "        [4.2380, 2.0130, 1.7145, 1.1075, 0.0000],\n",
      "        [1.5108, 0.9126, 3.9933, 3.2403, 0.3356]])\n"
     ]
    }
   ],
   "source": [
    "# 답 작성\n",
    "\n",
    "W = torch.randn(num_feature, 5)   # weight   - 입력은 4개\n",
    "b = torch.ones(1,5)       # bias의 차원은 해당 layer의 출력과 차원이 같아야 한다.(각 레이어의 출력 개수가 bias의 개수를 결정.)\n",
    "Z = torch.matmul(X_batch, W) + b\n",
    "A = ReLU(Z)  # 또는 \"torch.relu(Z)\"\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: \n",
    "\n",
    "- $x^{(3)}$ 를 입력으로하는 2번째 Neuron의 결과값을 출력하세요\n",
    "- 위에서 구한 `A[i, j]`  인덱싱을 통해서 출력하세요\n",
    "- `Python`의 인덱싱은 `0`부터 시작한다는 것을 주의하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer =  tensor(1.3657)\n"
     ]
    }
   ],
   "source": [
    "# 답 작성 -> 시험문제 100% !!!!!!!!!!!!!!!!!\n",
    "\n",
    "print('answer = ', A[2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Multi-Layer network\n",
    "\n",
    "- 3개의 layer가 있는 network를 구성합니다.\n",
    "- 2번째 layer의 입력 크기는 $k^{[1]}=16$, 출력 크기는 $k^{[2]}=6$\n",
    "- 마지막 layer의 출력은 $k^{[3]}=1$개의 neuron으로 구성\n",
    "- 각 layer의 연산값을 구하세요. \n",
    "  - 각 layer의 선형 변환 결과값은 `Z1`, `Z2`, `Z3`로 저장하세요\n",
    "  - 각 layer의 결과값은 `A1`, `A2`, `A3`로 저장하세요\n",
    "- 모든 weight는 Gaussian 랜덤 변수로 생성, bias는 1로 구성된 벡터로 생성함\n",
    "- 각 layer의 weight는 `W1`, `W2`, `W3`로하고, bias는 `b1`, `b2`, `b3`로 생성함\n",
    "- Activation 함수는 `ReLU`를 적용하세요\n",
    "- Loop 없이 행렬연산으로 구생하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답 작성\n",
    "\n",
    "W1= torch.randn(num_feature, 16)  # 첫번째 layer의 입력,출력\n",
    "W2= torch.randn(16, 6)            # 두번째 layer의 입력,출력\n",
    "W3= torch.randn(6, 1)             # 세번째 layer의 입력,출력\n",
    "b1= torch.ones(1,16)  # 각 레이어의 출력 개수가 bias의 개수를 결정\n",
    "b2= torch.ones(1,6)\n",
    "b3= torch.ones(1,1)\n",
    "\n",
    "Z1 = torch.matmul(X_batch, W1) + b1\n",
    "A1 = ReLU(Z1)    # 선형연산 후 활성함수 씌우기 -> 씌운 것을 다음 입력으로 할당.\n",
    "Z2 = torch.matmul(A1, W2) + b2     # 앞의 layer에서 ReLU까지 씌운 것을 새로운 입력으로 집어 넣는다!!!\n",
    "A2 = ReLU(Z2)\n",
    "Z3 = torch.matmul(A2, W3) + b3\n",
    "A3 = ReLU(Z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16])\n",
      "tensor([[ 2.0801,  1.7391,  2.2269, -0.7981, -0.7390,  0.0968, -1.8379,  1.8388,\n",
      "          1.2549,  2.0181,  3.2506,  2.0400,  1.7159,  2.5027,  1.6263,  1.1480],\n",
      "        [-0.0951,  1.7975,  3.6935,  3.0700, -0.8845,  1.5433, -1.0525, -1.8322,\n",
      "          3.2497,  2.0219, -3.8923, -0.6622, -0.8852,  2.7848,  3.2832,  4.4462],\n",
      "        [ 1.3614,  0.2582,  0.0568, -0.4358,  1.5147,  0.5280,  2.6339,  1.2636,\n",
      "          0.5098,  1.8655,  3.6205,  0.6451,  1.9524, -0.4146,  0.4182, -0.2648],\n",
      "        [ 1.6692,  0.0781,  1.9285,  2.4770,  2.2780,  0.9450,  1.4458,  1.2239,\n",
      "          1.8461,  0.4748, -1.9209,  1.3471, -0.3995,  0.3069,  1.7204,  0.2429],\n",
      "        [ 3.1308,  0.5200,  0.6067, -2.0824,  1.1086, -0.4654,  0.3823,  3.6084,\n",
      "          0.1007,  1.7009,  6.1254,  2.8840,  2.6837,  0.4640,  0.3486, -1.7262],\n",
      "        [ 0.4221,  0.2394, -2.8579, -1.0929,  2.5791,  1.0923,  5.1368,  1.7837,\n",
      "         -1.4512,  0.8618,  6.3659,  0.4396,  3.3769, -1.5827, -1.7181, -1.1916],\n",
      "        [ 1.9105,  0.3518,  0.7569, -0.1393,  1.4965,  0.3656,  1.5651,  1.8679,\n",
      "          0.7706,  1.4811,  2.8536,  1.4459,  1.5414,  0.1242,  0.7773, -0.4354],\n",
      "        [ 2.2050, -0.1125,  0.4940, -0.1629,  2.1633,  0.2527,  2.2805,  2.2271,\n",
      "          0.6503,  1.3516,  2.9996,  1.6007,  1.5271, -0.5321,  0.5921, -1.1990],\n",
      "        [-1.2452,  1.6971, -0.3023,  1.9095,  0.5553,  2.1923,  2.4091, -1.0349,\n",
      "          0.5515,  0.8222,  0.2617, -0.9196,  1.1913,  1.0824,  0.3538,  3.0097],\n",
      "        [-0.1046,  2.1360, -1.7997, -0.0858,  0.8617,  1.6524,  1.5332,  1.9430,\n",
      "         -1.3563, -0.3911,  4.5905,  1.5049,  2.9033,  1.4473, -1.2968,  0.9208]])\n"
     ]
    }
   ],
   "source": [
    "print(Z1.shape)\n",
    "print(Z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6])\n",
      "tensor([[ -4.7887,   5.3731,   9.0014,   0.4627,  12.3099,  -2.2157],\n",
      "        [  4.4594,  -3.6354, -14.5506,  -2.2102,  15.3791,   5.4080],\n",
      "        [ -5.3607,   7.4295,   7.5949,   0.5785,   4.2134,  -6.4505],\n",
      "        [ -0.3103,   3.3784,  -7.8630,  -0.1194,   7.8764,   0.0709],\n",
      "        [ -9.6758,  10.1579,  17.8718,   1.6869,   6.5051,  -7.3295],\n",
      "        [ -5.6529,  15.9200,  12.5567,   2.7773,  -4.2185, -13.9097],\n",
      "        [ -5.1472,   7.0292,   6.3847,   0.6831,   6.0662,  -4.8613],\n",
      "        [ -6.0453,   7.5192,   6.9782,   0.7942,   4.7135,  -5.5253],\n",
      "        [  3.3360,   7.8961,  -6.3540,   0.7474,   3.8727,  -5.0218],\n",
      "        [ -0.7215,  17.5243,   7.0787,   3.7593,   0.3094, -11.2054]])\n"
     ]
    }
   ],
   "source": [
    "print(Z2.shape)\n",
    "print(Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "tensor([[19.4468],\n",
      "        [-5.8658],\n",
      "        [16.6252],\n",
      "        [ 3.2753],\n",
      "        [28.4331],\n",
      "        [24.3355],\n",
      "        [16.4031],\n",
      "        [16.8553],\n",
      "        [ 5.6916],\n",
      "        [23.6182]])\n"
     ]
    }
   ],
   "source": [
    "print(Z3.shape)\n",
    "print(Z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "- 위에서 구한 `A3[i,j]`의 인덱싱을 통해서 $h_\\theta(x^{(3)})$ 을 출력하세요\n",
    "- 역시 `python`인덱싱은 `0` 부터 시작한다는 것을 주의하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_theta(x3) =  tensor([16.6252])\n"
     ]
    }
   ],
   "source": [
    "# 답 작성\n",
    "\n",
    "print('h_theta(x3) = ', A3[2,:])  # 또는 A3[2,0]도 될듯..? 마지막 출력이 1개(0번째)이니까..! # A3에서 input X3를 넣었을 때의 출력값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6\n",
    "위에서 공부한 것을 함수로 만들어 보도록 하겠습니다.\n",
    "아래 한 layer의 선형 변환을 연산하는 class를 만들어 보도록 하죠.\n",
    "\n",
    "- Class는 `my_linear_layer()`\n",
    "  - `__init__(self, n_input, n_output)` 함수:\n",
    "    - `self.W` 변수 초기화: Weight 행렬 `self.W`를 램덤 Gaussian 생성 (차원에 맞는...)\n",
    "    - `self.b` 변수 초기화: bias 벡터 `self.b`를 모두 `1`인 벡터 생성 (차원에 맞는...)\n",
    "  - `forward(A)` 함수:\n",
    "    - 입력: `A`는 sample batch $X$ 또는 전 layer에서 들어오는 입력 batch $A^{[\\ell-1]}$을 입력하는 자리\n",
    "    - return 값\n",
    "      - `Z` 변수는 $A^{[\\ell-1]}$의 선형 변환 값, 즉 $Z^{[\\ell]}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답 작성\n",
    "class my_linear_layer():\n",
    "    def __init__(self, n_input, n_output):\n",
    "        self.W = torch.randn(n_input, n_output) \n",
    "        self.b = torch.ones(1, n_output)\n",
    "    \n",
    "    def forward(self,A):   # 위의 self.W, self.b를 받기 위해서 변수에 self를 넣어줌.\n",
    "        Z = torch.matmul(A, self.W) + self.b\n",
    "        \n",
    "        return Z\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "답을 확인하기 위해서 `n_input=num_feature`과 `n_output = 5` 인 `my_linear_layer` instance 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8611,  2.0314,  0.2790, -0.8108,  0.4521],\n",
       "        [-0.6882,  0.4047,  7.4322, -2.2059,  4.0892],\n",
       "        [ 1.5038, -0.0691, -1.3451, -0.9375, -0.6050],\n",
       "        [ 1.8934,  0.8772,  1.6342,  4.3204,  2.8054],\n",
       "        [ 2.4192,  1.7168, -4.2347,  1.2639, -1.3634],\n",
       "        [ 1.5758, -0.2517, -4.0956, -0.4636, -2.9866],\n",
       "        [ 1.7973,  0.7150, -1.2511,  0.9306,  0.1003],\n",
       "        [ 2.2822,  0.5233, -2.1322,  1.8213, -0.0287],\n",
       "        [-0.5122,  0.0725,  3.3500, -1.8568,  0.6430],\n",
       "        [ 0.4624,  2.1736, -1.5206,  1.5113, -1.7080]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mll = my_linear_layer(num_feature, 5)\n",
    "mll.forward(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2488, -1.2320,  0.6257, -1.2231, -0.1292],\n",
      "        [-0.0546,  0.4083,  1.1264,  1.9351,  1.0077],\n",
      "        [ 1.0046, -0.4335, -1.2426,  1.2846,  0.2438],\n",
      "        [ 0.5304, -0.0145, -2.2357,  1.4660, -1.2191]])\n",
      "tensor([[1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(mll.W)\n",
    "print(mll.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Linear Layer with `torch.nn`\n",
    "위에서 수행한 작업을 `pytorch`에서는 `torch.nn.Linear`라는 명령어로 쉽게 구현할 수 있습니다.\n",
    "아래 예제를 보도록 하죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4486,  0.3506, -0.7073,  0.1749, -0.5759],\n",
       "        [-0.4611, -0.6973, -0.9464, -0.8248, -0.7580],\n",
       "        [-0.4073,  0.6585, -0.8455, -0.1103, -0.5166],\n",
       "        [ 0.8864,  0.4021,  0.3734, -0.0394, -0.0616],\n",
       "        [-0.9732,  1.1514, -0.3514,  0.5042, -0.3276],\n",
       "        [-0.1525,  0.8271, -0.8058,  0.4859, -0.3310],\n",
       "        [-0.2760,  0.7026, -0.4114,  0.0769, -0.3582],\n",
       "        [ 0.0247,  0.9050, -0.2421,  0.0966, -0.2674],\n",
       "        [-0.1999, -0.4030, -0.9645, -0.0435, -0.5377],\n",
       "        [-0.5489,  0.1861, -0.2480,  1.1752, -0.1287]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "#W = torch.randn(num_feature, 5)\n",
    "L1 = nn.Linear(num_feature, 5)\n",
    "Zll = L1(X_batch)\n",
    "Zll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0259,  0.1532,  0.7126,  0.0986, -0.2395],\n",
       "        [ 2.1053,  1.3770, -1.3077, -0.5116, -2.4858],\n",
       "        [ 4.2555,  1.3657,  1.1774, -0.2346,  1.5884],\n",
       "        [-2.7106,  0.9112, -0.0279,  1.3927,  3.2091],\n",
       "        [ 1.6620,  0.1195,  1.6564,  0.4289,  2.8937],\n",
       "        [ 5.5180,  1.8361,  3.4116,  1.1107,  2.2406],\n",
       "        [ 1.7660,  0.8838,  0.9816,  0.3584,  2.2296],\n",
       "        [ 1.2537,  0.9196,  1.0118,  0.4391,  3.2824],\n",
       "        [ 4.2380,  2.0130,  1.7145,  1.1075, -1.5468],\n",
       "        [ 1.5108,  0.9126,  3.9933,  3.2403,  0.3356]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1.weight = nn.Parameter(W.T)\n",
    "L1.bias.data.fill_(1.0)\n",
    "Zll2 = L1(X_batch)\n",
    "Zll2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0259,  0.1532,  0.7126,  0.0986, -0.2395],\n",
       "        [ 2.1053,  1.3770, -1.3077, -0.5116, -2.4858],\n",
       "        [ 4.2555,  1.3657,  1.1774, -0.2346,  1.5884],\n",
       "        [-2.7106,  0.9112, -0.0279,  1.3927,  3.2091],\n",
       "        [ 1.6620,  0.1195,  1.6564,  0.4289,  2.8937],\n",
       "        [ 5.5180,  1.8361,  3.4116,  1.1107,  2.2406],\n",
       "        [ 1.7660,  0.8838,  0.9816,  0.3584,  2.2296],\n",
       "        [ 1.2537,  0.9196,  1.0118,  0.4391,  3.2824],\n",
       "        [ 4.2380,  2.0130,  1.7145,  1.1075, -1.5468],\n",
       "        [ 1.5108,  0.9126,  3.9933,  3.2403,  0.3356]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
