{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36a2146",
   "metadata": {},
   "source": [
    "이름: 이예주\n",
    "\n",
    "학번: 20193237"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c04dd",
   "metadata": {},
   "source": [
    "# Neural networks with PyTorch\n",
    "\n",
    "Pytorch의 `nn.module`을 활용하여 만드는 유용한 방법을 학습합니다.\n",
    "\n",
    "<div style=\"text-align:center\"><img src='https://drive.google.com/uc?export=download&id=1J2SeiPpVJs1-ML2BdLrcxkGGmHpRxIVE' width=\"250\" height=\"200\"> \n",
    "\n",
    "### Lego block coding! </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd06918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2a3eb737ef0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn  # ReLU, 시그모이드 함수들 존재~~\n",
    "import torch.nn.functional as F # \"\"\n",
    "from collections import OrderedDict # 순서가 있는 딕셔너리(파이썬 라이브러리)\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa5a511",
   "metadata": {},
   "source": [
    "layer별로 Linear연산을 할 수 있어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605a354",
   "metadata": {},
   "source": [
    "`nn.Linear`: $Z^{[\\ell]} = A^{[\\ell-1]}W^T+b$\n",
    "연산.\n",
    "\n",
    "해당 layer의 \n",
    "\n",
    "- 입력 차원 `n_input=30`  # 유닛 수(동그라미 수)\n",
    "- 출력 차원 `n_output=60`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9881f3",
   "metadata": {},
   "source": [
    "--- 메모 ---   \n",
    "```python\n",
    "Z[ℓ] = A[ℓ-1] * W.T + b\n",
    "      = (batch_size, n_input) * (n_output, n_input).T + (n_output,)\n",
    "      = (batch_size, n_output)\n",
    "\n",
    "```\n",
    "**`batch_size`는 입력 데이터의 샘플 수**   \n",
    "**W의 크기는 (n_input, n_output)**   \n",
    "<br><br>\n",
    "해당 layer의 동그라미 수가 30개, 나가는 수가 60개   \n",
    "* Z의 차원   \n",
    "column의 개수가 60(출력겸 다음 layer의 입력)   \n",
    "row들의 개수는 batch(입력 데이터의 샘플)수   \n",
    "* A의 차원   \n",
    "열의 개수가 30(입력)   \n",
    "행의 수는 batch 수   \n",
    "* W의 차원   \n",
    "행60, 열30을 바꿔줬기 때문에(transpose)   \n",
    "행 : 30   \n",
    "열 : 60   \n",
    "* b의 차원   \n",
    "output 출력과 개수가 같아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe1c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of nn.linear\n",
    "# Linear 연산하는 함수\n",
    "linear_layer1 = nn.Linear(30, 60)  # nn.Linear(입력수, 출력수)  / 들어가는 동그라미 수, 나가는 동그라미 수\n",
    "\n",
    "# batch는(batch 수의 차원은) 언제나 동일하다.(학습 중에 일정하게 유지됨) \n",
    "\n",
    "# 입력받는 것의 shape 0번이 batch size라는 것을 알고 있기 때문에 batch size는 따로 입력 X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0560ee",
   "metadata": {},
   "source": [
    "`nn.Linear`는 W와 b값을 랜덤으로 초기화 해서 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4a14fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 60])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test sample\n",
    "A = torch.randn(60, 30)  # 배치사이즈는 60\n",
    "linear_layer1(A).shape  # (60, 30) x (30, 60) = (60, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bcbcbf",
   "metadata": {},
   "source": [
    "How to get the weights and bias of each `nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "214864da-24f2-454f-8f90-3db57cfefa91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0014,  0.0979, -0.1503,  ..., -0.1694, -0.1149, -0.0462],\n",
       "        [-0.0712,  0.1577, -0.1183,  ..., -0.1416, -0.1265, -0.0943],\n",
       "        [ 0.0826,  0.0734, -0.1081,  ..., -0.0836,  0.0702, -0.1081],\n",
       "        ...,\n",
       "        [ 0.1375,  0.0413, -0.0681,  ..., -0.0763,  0.1550,  0.0914],\n",
       "        [-0.1336,  0.1579, -0.0027,  ...,  0.1744,  0.0625, -0.0929],\n",
       "        [-0.1096, -0.0640, -0.0572,  ...,  0.0878,  0.0632, -0.0053]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac8531f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 30])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.weight.shape  # W의 transpose를 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c212bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.bias.shape  # bias는 1차원 / output layer가 60이니까..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e636fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of weights\n",
    "# weight를 다른 값으로 바꾸고 싶을 때 사용.\n",
    "linear_layer1.weight.data = torch.ones_like(linear_layer1.weight)  # torch.ones_like :입력 텐서와 동일한 모양(차원)의 텐서를 만들어주는데, 1로 채워줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0860b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a96b2d",
   "metadata": {},
   "source": [
    "### NN example\n",
    "\n",
    "- input units: 20\n",
    "- hidden layer: 30, 40\n",
    "- output units: 3\n",
    "- activation function: ReLU\n",
    "- output layer: No activation (마지막 layer만)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c09e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN construction\n",
    "\n",
    "class FCN(nn.Module):      # nn.Module : neural network모듈을 만들기 위한 베이스 모듈(class).\n",
    "    def __init__(self):    # 자동으로 실행되는 첫번째 함수\n",
    "        super().__init__() # nn.Module을 인스턴스로 불러온 것. 그리고 그것의 __init__함수를 실행시켜라.\n",
    "        \n",
    "        self.lin1 = nn.Linear(20, 30)\n",
    "        self.lin2 = nn.Linear(30, 40)\n",
    "        self.lin3 = nn.Linear(40, 3)\n",
    "        \n",
    "        self.relu = nn.ReLU(True)   # F.relu(True)\n",
    "        \n",
    "    # 위의 것들을 가지고 forward pass연산을 하는 것을 정의하는 함수(이름은 forward로 고정해야함)\n",
    "    # self를 해주는 이유 -> 위에 있는 함수를 불러와줘야하기 때문에.(다 일맥상통 함)\n",
    "    def forward(self, x):  # 입력 : x\n",
    "        x = self.lin1(x)   # x를 집어넣어서 Linear 연산을 한 다음에 x로 받는다\n",
    "        x = self.relu(x)   # relu 씌우기\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin3(x)   # 마지막 layer는 No activation.\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88009d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = torch.randn(60, 20)  # 입력 샘플(배치수:60)\n",
    "\n",
    "model = FCN()     # 인스턴스를 만들고\n",
    "# model(Xtrain)   # 데이터를 집어넣으면 forward연산이 된다 \n",
    "\n",
    "# model.lin1.weight.data = torch.ones(30,20)  # ones에는 weight의 차원에 맞춰서 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f646b76f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN(\n",
       "  (lin1): Linear(in_features=20, out_features=30, bias=True)\n",
       "  (lin2): Linear(in_features=30, out_features=40, bias=True)\n",
       "  (lin3): Linear(in_features=40, out_features=3, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "912f18e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.1542e-01, -1.5343e-01, -1.4473e-01,  6.0137e-02,  3.4798e-02,\n",
       "         -1.9658e-01, -1.6290e-01, -1.7801e-01,  1.7759e-01,  1.4619e-01,\n",
       "         -1.1770e-01,  3.3738e-02, -5.1431e-02,  1.9439e-01, -2.0253e-01,\n",
       "         -1.6313e-01, -1.8617e-01,  1.3013e-01, -1.1178e-01,  1.3318e-01],\n",
       "        [ 7.5586e-02, -1.0930e-01, -6.0237e-02, -9.0490e-02,  9.8650e-02,\n",
       "         -8.9674e-02,  7.5885e-02,  1.5974e-02, -1.7857e-01, -1.4056e-01,\n",
       "         -1.5504e-01,  1.1717e-01,  4.8451e-02, -5.6224e-03, -1.3314e-01,\n",
       "         -2.2155e-01, -7.9131e-02, -1.7114e-01, -2.2241e-01, -2.1327e-01],\n",
       "        [-1.4606e-01, -2.1738e-01,  6.9704e-02,  8.3668e-02,  9.0129e-02,\n",
       "         -2.2260e-01,  1.3827e-01,  4.3595e-02,  1.1434e-01, -5.4957e-02,\n",
       "          1.0738e-01, -2.1207e-01,  7.6162e-02,  1.7731e-01,  9.3997e-02,\n",
       "         -2.1077e-01, -6.2772e-02, -2.7768e-02,  1.1716e-01, -8.4613e-02],\n",
       "        [ 3.2161e-02, -7.8497e-02, -3.5522e-02, -4.4512e-03, -1.2840e-01,\n",
       "         -1.5520e-01,  1.8460e-01,  1.8787e-01, -6.7696e-03,  1.9355e-01,\n",
       "          1.7231e-01, -1.4085e-01, -8.8852e-02, -1.5358e-01,  7.2294e-02,\n",
       "          5.6551e-03, -3.0665e-02,  1.6602e-01, -8.7401e-02, -1.2238e-01],\n",
       "        [-1.5896e-01, -1.1376e-01,  2.1552e-01, -1.0872e-01, -6.1526e-02,\n",
       "          1.0078e-01, -1.7174e-01, -1.3572e-01,  1.6458e-01, -8.5763e-02,\n",
       "         -1.1603e-01, -2.8930e-02, -3.1862e-02,  4.0865e-02,  8.1979e-02,\n",
       "         -1.7241e-01,  2.0321e-01, -2.1259e-01,  7.5732e-02, -6.3092e-02],\n",
       "        [-9.4428e-02,  2.0917e-01, -2.1548e-01,  1.5271e-02,  1.4328e-01,\n",
       "          1.9990e-01, -1.8904e-01,  2.6941e-02,  2.2123e-01, -2.1903e-01,\n",
       "          5.4616e-02, -1.4011e-01,  3.6300e-02, -2.0227e-01,  1.1002e-01,\n",
       "         -4.9605e-02,  1.3364e-01, -3.3625e-03, -3.1873e-02, -5.4288e-02],\n",
       "        [-2.0474e-01, -9.4979e-02,  7.4261e-03, -1.7299e-01,  1.9624e-01,\n",
       "         -1.4543e-01, -8.7115e-02, -2.9903e-02, -1.8114e-01, -1.7667e-03,\n",
       "         -1.0768e-01, -1.8717e-01,  5.4847e-03,  1.9796e-01,  5.5063e-02,\n",
       "          1.8443e-01, -2.1867e-03, -7.8048e-02,  1.8022e-01, -1.1907e-01],\n",
       "        [-2.0686e-01,  4.8939e-02,  1.1144e-01, -1.3366e-01,  7.8702e-02,\n",
       "         -7.9332e-02, -2.7131e-02, -4.9511e-02, -4.7828e-02, -1.8194e-01,\n",
       "          5.4802e-02, -5.8818e-02,  1.7098e-01,  3.7323e-02, -1.8287e-01,\n",
       "         -1.7011e-01, -1.1654e-01,  1.0708e-01, -1.4437e-01, -1.0229e-01],\n",
       "        [ 4.0596e-02, -1.5503e-01, -1.1011e-01,  2.0276e-01, -1.1822e-01,\n",
       "         -7.4499e-02,  1.5992e-01,  5.0107e-02,  1.7551e-01, -1.9700e-01,\n",
       "          1.1177e-01, -3.4202e-02, -1.4325e-01, -9.5770e-02, -2.1629e-01,\n",
       "          1.5469e-01, -4.2906e-02, -2.1926e-01,  1.9123e-01,  1.4483e-01],\n",
       "        [ 9.2458e-02,  2.1885e-01,  2.0193e-01,  2.0897e-01,  2.0025e-01,\n",
       "          1.8172e-01,  5.8538e-02, -1.8726e-02,  8.5036e-03, -9.2926e-02,\n",
       "          1.0506e-01,  6.4780e-03,  5.2755e-02, -1.4404e-01, -8.4194e-02,\n",
       "          1.6764e-01,  2.1748e-02,  7.7165e-02, -1.9521e-01, -9.5754e-02],\n",
       "        [-5.6909e-03, -2.3464e-02,  1.4274e-01, -6.7481e-02,  1.8662e-01,\n",
       "         -4.3247e-02,  8.6972e-02, -1.5743e-01,  3.7954e-02, -2.1800e-01,\n",
       "         -1.9185e-01, -1.4311e-01,  6.2600e-02,  1.7331e-02,  2.1642e-02,\n",
       "         -2.4460e-02,  1.8789e-01,  2.1905e-01,  9.4963e-02,  1.7782e-01],\n",
       "        [-1.1051e-02, -8.2205e-02,  4.3471e-02,  1.4703e-01, -3.4559e-02,\n",
       "         -1.7169e-01,  7.3843e-02, -5.7001e-02, -1.0764e-01,  1.4124e-01,\n",
       "         -7.5897e-03, -1.2782e-01, -1.3380e-01,  2.1058e-01,  1.4544e-01,\n",
       "         -1.6034e-01,  1.0689e-04, -4.8377e-03,  1.0659e-01, -1.9151e-01],\n",
       "        [ 3.3233e-03,  7.4504e-02,  1.1379e-01,  2.3381e-02, -1.1084e-01,\n",
       "         -2.1467e-01,  1.2086e-01,  2.0414e-01, -6.7511e-02, -1.9983e-01,\n",
       "          6.5424e-02, -1.5677e-01, -1.4610e-01,  6.0952e-02, -2.1234e-02,\n",
       "         -1.5210e-02,  2.0056e-01, -3.9221e-02,  1.3611e-01, -6.9592e-02],\n",
       "        [ 1.6639e-01,  6.8216e-02,  1.9926e-01,  7.7989e-02, -7.9205e-02,\n",
       "          9.7460e-02,  1.9051e-01,  4.2381e-02,  1.0309e-01, -1.5731e-01,\n",
       "          1.2116e-01, -2.1137e-01,  1.9767e-01,  9.2355e-02,  1.4715e-02,\n",
       "         -2.0929e-01,  2.0334e-01,  1.4258e-01,  2.0614e-01,  8.7902e-02],\n",
       "        [ 7.5306e-02, -5.7922e-02, -9.1634e-02,  8.8638e-02,  7.7548e-02,\n",
       "         -7.2493e-03,  3.5567e-02,  7.6812e-02, -2.1608e-01, -1.9775e-01,\n",
       "         -5.9525e-02,  1.5229e-01,  1.0909e-01, -2.1342e-01, -6.7138e-02,\n",
       "         -2.3636e-02,  7.3442e-02,  1.8294e-01,  4.8906e-03, -1.7405e-01],\n",
       "        [ 1.3720e-02, -8.6348e-02, -1.3666e-01,  1.2385e-01, -3.0042e-02,\n",
       "          1.0566e-01,  1.7411e-01,  2.1976e-02,  1.2595e-01, -1.0561e-01,\n",
       "          1.2482e-01,  8.9331e-03,  1.4859e-01,  2.1378e-01, -1.6915e-01,\n",
       "          2.1227e-01,  1.7069e-01, -1.8013e-01,  6.5548e-02, -1.8204e-01],\n",
       "        [ 7.2535e-02,  3.4579e-02, -2.1346e-01,  8.1173e-02, -6.1420e-02,\n",
       "          2.2289e-01,  2.1824e-01,  7.7191e-02,  1.7626e-01,  8.0762e-02,\n",
       "          2.1155e-01, -2.0645e-02,  5.5265e-02, -3.6936e-02, -3.9105e-03,\n",
       "         -6.7455e-02, -8.8493e-02,  1.2935e-01,  9.5956e-02, -9.8709e-02],\n",
       "        [-1.9707e-01,  1.0933e-01, -2.0569e-01, -9.8949e-03, -1.9240e-01,\n",
       "          1.8723e-01,  1.8208e-01, -1.8377e-01, -2.0789e-01, -1.3221e-01,\n",
       "          6.1702e-02, -1.1182e-01, -5.3251e-02, -5.6680e-02,  4.1017e-02,\n",
       "         -1.0577e-01,  1.0170e-01, -1.6001e-01, -1.9904e-02,  1.8743e-01],\n",
       "        [-1.2347e-01,  1.7156e-01,  1.1638e-01, -7.2229e-02,  7.7539e-02,\n",
       "         -1.5106e-01,  1.7996e-02,  6.3339e-02,  1.9341e-01, -1.2513e-01,\n",
       "         -1.2717e-01, -1.1160e-01, -2.1783e-01,  4.5219e-02, -1.8509e-01,\n",
       "         -6.0026e-02, -9.9769e-02,  1.2502e-02,  6.7831e-02, -1.6249e-01],\n",
       "        [-6.6012e-02, -2.1497e-02, -1.2926e-01,  5.1656e-02,  8.2778e-02,\n",
       "         -1.5936e-01,  1.7583e-01,  1.5889e-01, -6.7177e-02,  3.3271e-02,\n",
       "          2.7226e-02, -3.0851e-02, -1.5007e-01,  1.8426e-01,  1.1004e-01,\n",
       "         -1.8686e-01, -1.7330e-01,  2.1264e-01,  3.5689e-02, -5.1373e-02],\n",
       "        [ 1.5155e-01,  6.4238e-02,  8.0268e-02,  1.7740e-01, -1.7452e-02,\n",
       "          2.1307e-01, -1.5707e-01, -1.3606e-01,  3.7888e-02, -2.7147e-02,\n",
       "          1.2760e-01, -6.0805e-02,  3.5162e-02, -4.6463e-02,  1.6787e-01,\n",
       "         -1.4667e-02,  3.4322e-03,  1.6381e-01,  2.1739e-01,  2.1396e-01],\n",
       "        [-2.1863e-01, -1.2635e-01, -4.8556e-02, -4.6162e-02,  1.2934e-01,\n",
       "          1.0099e-01, -1.8392e-02,  5.2972e-03,  4.7909e-02,  1.9276e-01,\n",
       "          1.1588e-01, -1.1279e-01, -1.1455e-01,  7.0715e-02, -2.2617e-02,\n",
       "         -1.4345e-01, -2.1642e-01,  3.2030e-02, -6.5298e-02, -1.3010e-01],\n",
       "        [ 4.2063e-02,  2.1774e-01,  1.8572e-01,  1.2749e-01, -1.6864e-01,\n",
       "          1.2762e-01,  1.1565e-01,  7.4605e-02, -5.5701e-02, -3.8937e-03,\n",
       "         -1.2128e-01, -5.0369e-02, -3.6809e-02, -3.1675e-02, -2.2253e-01,\n",
       "          7.8886e-02, -1.8295e-01, -1.6722e-01,  9.0867e-02, -1.8304e-01],\n",
       "        [ 2.1332e-01,  2.1411e-01, -7.1058e-02, -3.8537e-02, -1.7107e-01,\n",
       "          1.3824e-02,  4.0883e-02, -1.2266e-01,  7.0119e-02, -8.6055e-03,\n",
       "         -6.5915e-02, -7.7361e-04, -2.1675e-01, -1.5752e-01,  2.9598e-02,\n",
       "          2.1594e-01, -1.3254e-01,  1.4335e-01,  2.0464e-02, -9.6203e-02],\n",
       "        [ 2.4499e-04,  1.0720e-01,  2.6603e-02,  1.9794e-01, -4.2319e-02,\n",
       "         -1.4932e-01,  2.0206e-01, -6.3862e-02,  1.4188e-01,  1.4749e-01,\n",
       "          3.9602e-02, -1.8352e-01, -2.1952e-01,  1.0615e-01,  5.4670e-02,\n",
       "          1.3011e-01, -1.9709e-01, -3.0807e-02,  1.9320e-01,  1.1748e-01],\n",
       "        [ 1.8582e-01,  1.7556e-01, -5.4866e-02, -1.5227e-01, -1.3333e-01,\n",
       "          7.5943e-02, -8.8758e-02,  8.8681e-04,  3.5684e-02,  1.3881e-01,\n",
       "          1.2471e-01,  1.2762e-01,  9.0880e-03,  1.2624e-01, -1.0468e-01,\n",
       "         -8.9498e-02, -3.5404e-02, -1.2985e-01,  1.9946e-01, -3.4264e-02],\n",
       "        [ 6.5008e-02, -6.9521e-03, -3.0583e-03, -1.1918e-01, -1.6417e-02,\n",
       "         -2.0002e-01, -1.0693e-01,  1.7208e-01, -2.0973e-01,  3.6797e-02,\n",
       "          3.5623e-02, -2.1402e-01,  4.3674e-02,  1.0382e-01,  1.6195e-01,\n",
       "          4.1854e-02,  1.7479e-01,  8.4166e-03,  8.5199e-02,  6.6011e-02],\n",
       "        [-1.6487e-01, -4.9085e-02, -1.5747e-01,  1.7374e-01,  1.0667e-01,\n",
       "         -9.4833e-02, -4.0919e-02,  5.9693e-02, -1.9497e-01,  6.3140e-03,\n",
       "          1.0666e-01,  2.8018e-02, -2.0574e-02,  1.4376e-01,  9.7266e-02,\n",
       "         -1.9879e-01,  1.3160e-01, -1.7636e-01,  1.2561e-01,  1.8829e-01],\n",
       "        [ 2.0608e-02, -7.5573e-02, -1.1550e-01, -9.9290e-02, -4.1218e-02,\n",
       "         -9.2703e-02,  2.2198e-01, -9.6452e-02,  9.2839e-02, -2.0736e-01,\n",
       "          1.1334e-01,  1.9027e-01, -1.6030e-01, -5.7052e-02,  5.3294e-02,\n",
       "         -2.1404e-01, -1.4148e-01,  7.7767e-02,  2.1056e-01, -6.6801e-02],\n",
       "        [-2.0364e-03, -2.1586e-01, -2.1463e-01,  1.9447e-01,  1.8975e-01,\n",
       "         -6.8148e-02, -8.8418e-02, -1.4475e-02,  1.0375e-01, -2.1609e-01,\n",
       "         -1.2386e-01, -7.3717e-02, -9.7327e-02, -5.9071e-03,  1.6310e-01,\n",
       "          4.8822e-02, -2.0236e-01,  2.0681e-01,  1.8005e-01,  7.0404e-02]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lin1에 들어있는 weight들\n",
    "model.lin1.weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44761e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1542e-01, -1.5343e-01, -1.4473e-01,  6.0137e-02,  3.4798e-02,\n",
       "         -1.9658e-01, -1.6290e-01, -1.7801e-01,  1.7759e-01,  1.4619e-01,\n",
       "         -1.1770e-01,  3.3738e-02, -5.1431e-02,  1.9439e-01, -2.0253e-01,\n",
       "         -1.6313e-01, -1.8617e-01,  1.3013e-01, -1.1178e-01,  1.3318e-01],\n",
       "        [ 7.5586e-02, -1.0930e-01, -6.0237e-02, -9.0490e-02,  9.8650e-02,\n",
       "         -8.9674e-02,  7.5885e-02,  1.5974e-02, -1.7857e-01, -1.4056e-01,\n",
       "         -1.5504e-01,  1.1717e-01,  4.8451e-02, -5.6224e-03, -1.3314e-01,\n",
       "         -2.2155e-01, -7.9131e-02, -1.7114e-01, -2.2241e-01, -2.1327e-01],\n",
       "        [-1.4606e-01, -2.1738e-01,  6.9704e-02,  8.3668e-02,  9.0129e-02,\n",
       "         -2.2260e-01,  1.3827e-01,  4.3595e-02,  1.1434e-01, -5.4957e-02,\n",
       "          1.0738e-01, -2.1207e-01,  7.6162e-02,  1.7731e-01,  9.3997e-02,\n",
       "         -2.1077e-01, -6.2772e-02, -2.7768e-02,  1.1716e-01, -8.4613e-02],\n",
       "        [ 3.2161e-02, -7.8497e-02, -3.5522e-02, -4.4512e-03, -1.2840e-01,\n",
       "         -1.5520e-01,  1.8460e-01,  1.8787e-01, -6.7696e-03,  1.9355e-01,\n",
       "          1.7231e-01, -1.4085e-01, -8.8852e-02, -1.5358e-01,  7.2294e-02,\n",
       "          5.6551e-03, -3.0665e-02,  1.6602e-01, -8.7401e-02, -1.2238e-01],\n",
       "        [-1.5896e-01, -1.1376e-01,  2.1552e-01, -1.0872e-01, -6.1526e-02,\n",
       "          1.0078e-01, -1.7174e-01, -1.3572e-01,  1.6458e-01, -8.5763e-02,\n",
       "         -1.1603e-01, -2.8930e-02, -3.1862e-02,  4.0865e-02,  8.1979e-02,\n",
       "         -1.7241e-01,  2.0321e-01, -2.1259e-01,  7.5732e-02, -6.3092e-02],\n",
       "        [-9.4428e-02,  2.0917e-01, -2.1548e-01,  1.5271e-02,  1.4328e-01,\n",
       "          1.9990e-01, -1.8904e-01,  2.6941e-02,  2.2123e-01, -2.1903e-01,\n",
       "          5.4616e-02, -1.4011e-01,  3.6300e-02, -2.0227e-01,  1.1002e-01,\n",
       "         -4.9605e-02,  1.3364e-01, -3.3625e-03, -3.1873e-02, -5.4288e-02],\n",
       "        [-2.0474e-01, -9.4979e-02,  7.4261e-03, -1.7299e-01,  1.9624e-01,\n",
       "         -1.4543e-01, -8.7115e-02, -2.9903e-02, -1.8114e-01, -1.7667e-03,\n",
       "         -1.0768e-01, -1.8717e-01,  5.4847e-03,  1.9796e-01,  5.5063e-02,\n",
       "          1.8443e-01, -2.1867e-03, -7.8048e-02,  1.8022e-01, -1.1907e-01],\n",
       "        [-2.0686e-01,  4.8939e-02,  1.1144e-01, -1.3366e-01,  7.8702e-02,\n",
       "         -7.9332e-02, -2.7131e-02, -4.9511e-02, -4.7828e-02, -1.8194e-01,\n",
       "          5.4802e-02, -5.8818e-02,  1.7098e-01,  3.7323e-02, -1.8287e-01,\n",
       "         -1.7011e-01, -1.1654e-01,  1.0708e-01, -1.4437e-01, -1.0229e-01],\n",
       "        [ 4.0596e-02, -1.5503e-01, -1.1011e-01,  2.0276e-01, -1.1822e-01,\n",
       "         -7.4499e-02,  1.5992e-01,  5.0107e-02,  1.7551e-01, -1.9700e-01,\n",
       "          1.1177e-01, -3.4202e-02, -1.4325e-01, -9.5770e-02, -2.1629e-01,\n",
       "          1.5469e-01, -4.2906e-02, -2.1926e-01,  1.9123e-01,  1.4483e-01],\n",
       "        [ 9.2458e-02,  2.1885e-01,  2.0193e-01,  2.0897e-01,  2.0025e-01,\n",
       "          1.8172e-01,  5.8538e-02, -1.8726e-02,  8.5036e-03, -9.2926e-02,\n",
       "          1.0506e-01,  6.4780e-03,  5.2755e-02, -1.4404e-01, -8.4194e-02,\n",
       "          1.6764e-01,  2.1748e-02,  7.7165e-02, -1.9521e-01, -9.5754e-02],\n",
       "        [-5.6909e-03, -2.3464e-02,  1.4274e-01, -6.7481e-02,  1.8662e-01,\n",
       "         -4.3247e-02,  8.6972e-02, -1.5743e-01,  3.7954e-02, -2.1800e-01,\n",
       "         -1.9185e-01, -1.4311e-01,  6.2600e-02,  1.7331e-02,  2.1642e-02,\n",
       "         -2.4460e-02,  1.8789e-01,  2.1905e-01,  9.4963e-02,  1.7782e-01],\n",
       "        [-1.1051e-02, -8.2205e-02,  4.3471e-02,  1.4703e-01, -3.4559e-02,\n",
       "         -1.7169e-01,  7.3843e-02, -5.7001e-02, -1.0764e-01,  1.4124e-01,\n",
       "         -7.5897e-03, -1.2782e-01, -1.3380e-01,  2.1058e-01,  1.4544e-01,\n",
       "         -1.6034e-01,  1.0689e-04, -4.8377e-03,  1.0659e-01, -1.9151e-01],\n",
       "        [ 3.3233e-03,  7.4504e-02,  1.1379e-01,  2.3381e-02, -1.1084e-01,\n",
       "         -2.1467e-01,  1.2086e-01,  2.0414e-01, -6.7511e-02, -1.9983e-01,\n",
       "          6.5424e-02, -1.5677e-01, -1.4610e-01,  6.0952e-02, -2.1234e-02,\n",
       "         -1.5210e-02,  2.0056e-01, -3.9221e-02,  1.3611e-01, -6.9592e-02],\n",
       "        [ 1.6639e-01,  6.8216e-02,  1.9926e-01,  7.7989e-02, -7.9205e-02,\n",
       "          9.7460e-02,  1.9051e-01,  4.2381e-02,  1.0309e-01, -1.5731e-01,\n",
       "          1.2116e-01, -2.1137e-01,  1.9767e-01,  9.2355e-02,  1.4715e-02,\n",
       "         -2.0929e-01,  2.0334e-01,  1.4258e-01,  2.0614e-01,  8.7902e-02],\n",
       "        [ 7.5306e-02, -5.7922e-02, -9.1634e-02,  8.8638e-02,  7.7548e-02,\n",
       "         -7.2493e-03,  3.5567e-02,  7.6812e-02, -2.1608e-01, -1.9775e-01,\n",
       "         -5.9525e-02,  1.5229e-01,  1.0909e-01, -2.1342e-01, -6.7138e-02,\n",
       "         -2.3636e-02,  7.3442e-02,  1.8294e-01,  4.8906e-03, -1.7405e-01],\n",
       "        [ 1.3720e-02, -8.6348e-02, -1.3666e-01,  1.2385e-01, -3.0042e-02,\n",
       "          1.0566e-01,  1.7411e-01,  2.1976e-02,  1.2595e-01, -1.0561e-01,\n",
       "          1.2482e-01,  8.9331e-03,  1.4859e-01,  2.1378e-01, -1.6915e-01,\n",
       "          2.1227e-01,  1.7069e-01, -1.8013e-01,  6.5548e-02, -1.8204e-01],\n",
       "        [ 7.2535e-02,  3.4579e-02, -2.1346e-01,  8.1173e-02, -6.1420e-02,\n",
       "          2.2289e-01,  2.1824e-01,  7.7191e-02,  1.7626e-01,  8.0762e-02,\n",
       "          2.1155e-01, -2.0645e-02,  5.5265e-02, -3.6936e-02, -3.9105e-03,\n",
       "         -6.7455e-02, -8.8493e-02,  1.2935e-01,  9.5956e-02, -9.8709e-02],\n",
       "        [-1.9707e-01,  1.0933e-01, -2.0569e-01, -9.8949e-03, -1.9240e-01,\n",
       "          1.8723e-01,  1.8208e-01, -1.8377e-01, -2.0789e-01, -1.3221e-01,\n",
       "          6.1702e-02, -1.1182e-01, -5.3251e-02, -5.6680e-02,  4.1017e-02,\n",
       "         -1.0577e-01,  1.0170e-01, -1.6001e-01, -1.9904e-02,  1.8743e-01],\n",
       "        [-1.2347e-01,  1.7156e-01,  1.1638e-01, -7.2229e-02,  7.7539e-02,\n",
       "         -1.5106e-01,  1.7996e-02,  6.3339e-02,  1.9341e-01, -1.2513e-01,\n",
       "         -1.2717e-01, -1.1160e-01, -2.1783e-01,  4.5219e-02, -1.8509e-01,\n",
       "         -6.0026e-02, -9.9769e-02,  1.2502e-02,  6.7831e-02, -1.6249e-01],\n",
       "        [-6.6012e-02, -2.1497e-02, -1.2926e-01,  5.1656e-02,  8.2778e-02,\n",
       "         -1.5936e-01,  1.7583e-01,  1.5889e-01, -6.7177e-02,  3.3271e-02,\n",
       "          2.7226e-02, -3.0851e-02, -1.5007e-01,  1.8426e-01,  1.1004e-01,\n",
       "         -1.8686e-01, -1.7330e-01,  2.1264e-01,  3.5689e-02, -5.1373e-02],\n",
       "        [ 1.5155e-01,  6.4238e-02,  8.0268e-02,  1.7740e-01, -1.7452e-02,\n",
       "          2.1307e-01, -1.5707e-01, -1.3606e-01,  3.7888e-02, -2.7147e-02,\n",
       "          1.2760e-01, -6.0805e-02,  3.5162e-02, -4.6463e-02,  1.6787e-01,\n",
       "         -1.4667e-02,  3.4322e-03,  1.6381e-01,  2.1739e-01,  2.1396e-01],\n",
       "        [-2.1863e-01, -1.2635e-01, -4.8556e-02, -4.6162e-02,  1.2934e-01,\n",
       "          1.0099e-01, -1.8392e-02,  5.2972e-03,  4.7909e-02,  1.9276e-01,\n",
       "          1.1588e-01, -1.1279e-01, -1.1455e-01,  7.0715e-02, -2.2617e-02,\n",
       "         -1.4345e-01, -2.1642e-01,  3.2030e-02, -6.5298e-02, -1.3010e-01],\n",
       "        [ 4.2063e-02,  2.1774e-01,  1.8572e-01,  1.2749e-01, -1.6864e-01,\n",
       "          1.2762e-01,  1.1565e-01,  7.4605e-02, -5.5701e-02, -3.8937e-03,\n",
       "         -1.2128e-01, -5.0369e-02, -3.6809e-02, -3.1675e-02, -2.2253e-01,\n",
       "          7.8886e-02, -1.8295e-01, -1.6722e-01,  9.0867e-02, -1.8304e-01],\n",
       "        [ 2.1332e-01,  2.1411e-01, -7.1058e-02, -3.8537e-02, -1.7107e-01,\n",
       "          1.3824e-02,  4.0883e-02, -1.2266e-01,  7.0119e-02, -8.6055e-03,\n",
       "         -6.5915e-02, -7.7361e-04, -2.1675e-01, -1.5752e-01,  2.9598e-02,\n",
       "          2.1594e-01, -1.3254e-01,  1.4335e-01,  2.0464e-02, -9.6203e-02],\n",
       "        [ 2.4499e-04,  1.0720e-01,  2.6603e-02,  1.9794e-01, -4.2319e-02,\n",
       "         -1.4932e-01,  2.0206e-01, -6.3862e-02,  1.4188e-01,  1.4749e-01,\n",
       "          3.9602e-02, -1.8352e-01, -2.1952e-01,  1.0615e-01,  5.4670e-02,\n",
       "          1.3011e-01, -1.9709e-01, -3.0807e-02,  1.9320e-01,  1.1748e-01],\n",
       "        [ 1.8582e-01,  1.7556e-01, -5.4866e-02, -1.5227e-01, -1.3333e-01,\n",
       "          7.5943e-02, -8.8758e-02,  8.8681e-04,  3.5684e-02,  1.3881e-01,\n",
       "          1.2471e-01,  1.2762e-01,  9.0880e-03,  1.2624e-01, -1.0468e-01,\n",
       "         -8.9498e-02, -3.5404e-02, -1.2985e-01,  1.9946e-01, -3.4264e-02],\n",
       "        [ 6.5008e-02, -6.9521e-03, -3.0583e-03, -1.1918e-01, -1.6417e-02,\n",
       "         -2.0002e-01, -1.0693e-01,  1.7208e-01, -2.0973e-01,  3.6797e-02,\n",
       "          3.5623e-02, -2.1402e-01,  4.3674e-02,  1.0382e-01,  1.6195e-01,\n",
       "          4.1854e-02,  1.7479e-01,  8.4166e-03,  8.5199e-02,  6.6011e-02],\n",
       "        [-1.6487e-01, -4.9085e-02, -1.5747e-01,  1.7374e-01,  1.0667e-01,\n",
       "         -9.4833e-02, -4.0919e-02,  5.9693e-02, -1.9497e-01,  6.3140e-03,\n",
       "          1.0666e-01,  2.8018e-02, -2.0574e-02,  1.4376e-01,  9.7266e-02,\n",
       "         -1.9879e-01,  1.3160e-01, -1.7636e-01,  1.2561e-01,  1.8829e-01],\n",
       "        [ 2.0608e-02, -7.5573e-02, -1.1550e-01, -9.9290e-02, -4.1218e-02,\n",
       "         -9.2703e-02,  2.2198e-01, -9.6452e-02,  9.2839e-02, -2.0736e-01,\n",
       "          1.1334e-01,  1.9027e-01, -1.6030e-01, -5.7052e-02,  5.3294e-02,\n",
       "         -2.1404e-01, -1.4148e-01,  7.7767e-02,  2.1056e-01, -6.6801e-02],\n",
       "        [-2.0364e-03, -2.1586e-01, -2.1463e-01,  1.9447e-01,  1.8975e-01,\n",
       "         -6.8148e-02, -8.8418e-02, -1.4475e-02,  1.0375e-01, -2.1609e-01,\n",
       "         -1.2386e-01, -7.3717e-02, -9.7327e-02, -5.9071e-03,  1.6310e-01,\n",
       "          4.8822e-02, -2.0236e-01,  2.0681e-01,  1.8005e-01,  7.0404e-02]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그 텐서를 보는 것 (30x20)\n",
    "model.lin1.weight.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15c92d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6bbcfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1095,  0.0836,  0.1833, -0.2151, -0.1392, -0.0284, -0.0425, -0.1485,\n",
       "        -0.2144, -0.1708, -0.0347,  0.1898,  0.1482,  0.0645, -0.1373, -0.0900,\n",
       "         0.0215, -0.2073, -0.0102,  0.0455,  0.2022, -0.0587,  0.2195,  0.0169,\n",
       "        -0.1551,  0.2113,  0.1939, -0.1767, -0.1297,  0.1069],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bias\n",
    "model.lin1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a24fa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.1542e-01, -1.5343e-01, -1.4473e-01,  6.0137e-02,  3.4798e-02,\n",
      "         -1.9658e-01, -1.6290e-01, -1.7801e-01,  1.7759e-01,  1.4619e-01,\n",
      "         -1.1770e-01,  3.3738e-02, -5.1431e-02,  1.9439e-01, -2.0253e-01,\n",
      "         -1.6313e-01, -1.8617e-01,  1.3013e-01, -1.1178e-01,  1.3318e-01],\n",
      "        [ 7.5586e-02, -1.0930e-01, -6.0237e-02, -9.0490e-02,  9.8650e-02,\n",
      "         -8.9674e-02,  7.5885e-02,  1.5974e-02, -1.7857e-01, -1.4056e-01,\n",
      "         -1.5504e-01,  1.1717e-01,  4.8451e-02, -5.6224e-03, -1.3314e-01,\n",
      "         -2.2155e-01, -7.9131e-02, -1.7114e-01, -2.2241e-01, -2.1327e-01],\n",
      "        [-1.4606e-01, -2.1738e-01,  6.9704e-02,  8.3668e-02,  9.0129e-02,\n",
      "         -2.2260e-01,  1.3827e-01,  4.3595e-02,  1.1434e-01, -5.4957e-02,\n",
      "          1.0738e-01, -2.1207e-01,  7.6162e-02,  1.7731e-01,  9.3997e-02,\n",
      "         -2.1077e-01, -6.2772e-02, -2.7768e-02,  1.1716e-01, -8.4613e-02],\n",
      "        [ 3.2161e-02, -7.8497e-02, -3.5522e-02, -4.4512e-03, -1.2840e-01,\n",
      "         -1.5520e-01,  1.8460e-01,  1.8787e-01, -6.7696e-03,  1.9355e-01,\n",
      "          1.7231e-01, -1.4085e-01, -8.8852e-02, -1.5358e-01,  7.2294e-02,\n",
      "          5.6551e-03, -3.0665e-02,  1.6602e-01, -8.7401e-02, -1.2238e-01],\n",
      "        [-1.5896e-01, -1.1376e-01,  2.1552e-01, -1.0872e-01, -6.1526e-02,\n",
      "          1.0078e-01, -1.7174e-01, -1.3572e-01,  1.6458e-01, -8.5763e-02,\n",
      "         -1.1603e-01, -2.8930e-02, -3.1862e-02,  4.0865e-02,  8.1979e-02,\n",
      "         -1.7241e-01,  2.0321e-01, -2.1259e-01,  7.5732e-02, -6.3092e-02],\n",
      "        [-9.4428e-02,  2.0917e-01, -2.1548e-01,  1.5271e-02,  1.4328e-01,\n",
      "          1.9990e-01, -1.8904e-01,  2.6941e-02,  2.2123e-01, -2.1903e-01,\n",
      "          5.4616e-02, -1.4011e-01,  3.6300e-02, -2.0227e-01,  1.1002e-01,\n",
      "         -4.9605e-02,  1.3364e-01, -3.3625e-03, -3.1873e-02, -5.4288e-02],\n",
      "        [-2.0474e-01, -9.4979e-02,  7.4261e-03, -1.7299e-01,  1.9624e-01,\n",
      "         -1.4543e-01, -8.7115e-02, -2.9903e-02, -1.8114e-01, -1.7667e-03,\n",
      "         -1.0768e-01, -1.8717e-01,  5.4847e-03,  1.9796e-01,  5.5063e-02,\n",
      "          1.8443e-01, -2.1867e-03, -7.8048e-02,  1.8022e-01, -1.1907e-01],\n",
      "        [-2.0686e-01,  4.8939e-02,  1.1144e-01, -1.3366e-01,  7.8702e-02,\n",
      "         -7.9332e-02, -2.7131e-02, -4.9511e-02, -4.7828e-02, -1.8194e-01,\n",
      "          5.4802e-02, -5.8818e-02,  1.7098e-01,  3.7323e-02, -1.8287e-01,\n",
      "         -1.7011e-01, -1.1654e-01,  1.0708e-01, -1.4437e-01, -1.0229e-01],\n",
      "        [ 4.0596e-02, -1.5503e-01, -1.1011e-01,  2.0276e-01, -1.1822e-01,\n",
      "         -7.4499e-02,  1.5992e-01,  5.0107e-02,  1.7551e-01, -1.9700e-01,\n",
      "          1.1177e-01, -3.4202e-02, -1.4325e-01, -9.5770e-02, -2.1629e-01,\n",
      "          1.5469e-01, -4.2906e-02, -2.1926e-01,  1.9123e-01,  1.4483e-01],\n",
      "        [ 9.2458e-02,  2.1885e-01,  2.0193e-01,  2.0897e-01,  2.0025e-01,\n",
      "          1.8172e-01,  5.8538e-02, -1.8726e-02,  8.5036e-03, -9.2926e-02,\n",
      "          1.0506e-01,  6.4780e-03,  5.2755e-02, -1.4404e-01, -8.4194e-02,\n",
      "          1.6764e-01,  2.1748e-02,  7.7165e-02, -1.9521e-01, -9.5754e-02],\n",
      "        [-5.6909e-03, -2.3464e-02,  1.4274e-01, -6.7481e-02,  1.8662e-01,\n",
      "         -4.3247e-02,  8.6972e-02, -1.5743e-01,  3.7954e-02, -2.1800e-01,\n",
      "         -1.9185e-01, -1.4311e-01,  6.2600e-02,  1.7331e-02,  2.1642e-02,\n",
      "         -2.4460e-02,  1.8789e-01,  2.1905e-01,  9.4963e-02,  1.7782e-01],\n",
      "        [-1.1051e-02, -8.2205e-02,  4.3471e-02,  1.4703e-01, -3.4559e-02,\n",
      "         -1.7169e-01,  7.3843e-02, -5.7001e-02, -1.0764e-01,  1.4124e-01,\n",
      "         -7.5897e-03, -1.2782e-01, -1.3380e-01,  2.1058e-01,  1.4544e-01,\n",
      "         -1.6034e-01,  1.0689e-04, -4.8377e-03,  1.0659e-01, -1.9151e-01],\n",
      "        [ 3.3233e-03,  7.4504e-02,  1.1379e-01,  2.3381e-02, -1.1084e-01,\n",
      "         -2.1467e-01,  1.2086e-01,  2.0414e-01, -6.7511e-02, -1.9983e-01,\n",
      "          6.5424e-02, -1.5677e-01, -1.4610e-01,  6.0952e-02, -2.1234e-02,\n",
      "         -1.5210e-02,  2.0056e-01, -3.9221e-02,  1.3611e-01, -6.9592e-02],\n",
      "        [ 1.6639e-01,  6.8216e-02,  1.9926e-01,  7.7989e-02, -7.9205e-02,\n",
      "          9.7460e-02,  1.9051e-01,  4.2381e-02,  1.0309e-01, -1.5731e-01,\n",
      "          1.2116e-01, -2.1137e-01,  1.9767e-01,  9.2355e-02,  1.4715e-02,\n",
      "         -2.0929e-01,  2.0334e-01,  1.4258e-01,  2.0614e-01,  8.7902e-02],\n",
      "        [ 7.5306e-02, -5.7922e-02, -9.1634e-02,  8.8638e-02,  7.7548e-02,\n",
      "         -7.2493e-03,  3.5567e-02,  7.6812e-02, -2.1608e-01, -1.9775e-01,\n",
      "         -5.9525e-02,  1.5229e-01,  1.0909e-01, -2.1342e-01, -6.7138e-02,\n",
      "         -2.3636e-02,  7.3442e-02,  1.8294e-01,  4.8906e-03, -1.7405e-01],\n",
      "        [ 1.3720e-02, -8.6348e-02, -1.3666e-01,  1.2385e-01, -3.0042e-02,\n",
      "          1.0566e-01,  1.7411e-01,  2.1976e-02,  1.2595e-01, -1.0561e-01,\n",
      "          1.2482e-01,  8.9331e-03,  1.4859e-01,  2.1378e-01, -1.6915e-01,\n",
      "          2.1227e-01,  1.7069e-01, -1.8013e-01,  6.5548e-02, -1.8204e-01],\n",
      "        [ 7.2535e-02,  3.4579e-02, -2.1346e-01,  8.1173e-02, -6.1420e-02,\n",
      "          2.2289e-01,  2.1824e-01,  7.7191e-02,  1.7626e-01,  8.0762e-02,\n",
      "          2.1155e-01, -2.0645e-02,  5.5265e-02, -3.6936e-02, -3.9105e-03,\n",
      "         -6.7455e-02, -8.8493e-02,  1.2935e-01,  9.5956e-02, -9.8709e-02],\n",
      "        [-1.9707e-01,  1.0933e-01, -2.0569e-01, -9.8949e-03, -1.9240e-01,\n",
      "          1.8723e-01,  1.8208e-01, -1.8377e-01, -2.0789e-01, -1.3221e-01,\n",
      "          6.1702e-02, -1.1182e-01, -5.3251e-02, -5.6680e-02,  4.1017e-02,\n",
      "         -1.0577e-01,  1.0170e-01, -1.6001e-01, -1.9904e-02,  1.8743e-01],\n",
      "        [-1.2347e-01,  1.7156e-01,  1.1638e-01, -7.2229e-02,  7.7539e-02,\n",
      "         -1.5106e-01,  1.7996e-02,  6.3339e-02,  1.9341e-01, -1.2513e-01,\n",
      "         -1.2717e-01, -1.1160e-01, -2.1783e-01,  4.5219e-02, -1.8509e-01,\n",
      "         -6.0026e-02, -9.9769e-02,  1.2502e-02,  6.7831e-02, -1.6249e-01],\n",
      "        [-6.6012e-02, -2.1497e-02, -1.2926e-01,  5.1656e-02,  8.2778e-02,\n",
      "         -1.5936e-01,  1.7583e-01,  1.5889e-01, -6.7177e-02,  3.3271e-02,\n",
      "          2.7226e-02, -3.0851e-02, -1.5007e-01,  1.8426e-01,  1.1004e-01,\n",
      "         -1.8686e-01, -1.7330e-01,  2.1264e-01,  3.5689e-02, -5.1373e-02],\n",
      "        [ 1.5155e-01,  6.4238e-02,  8.0268e-02,  1.7740e-01, -1.7452e-02,\n",
      "          2.1307e-01, -1.5707e-01, -1.3606e-01,  3.7888e-02, -2.7147e-02,\n",
      "          1.2760e-01, -6.0805e-02,  3.5162e-02, -4.6463e-02,  1.6787e-01,\n",
      "         -1.4667e-02,  3.4322e-03,  1.6381e-01,  2.1739e-01,  2.1396e-01],\n",
      "        [-2.1863e-01, -1.2635e-01, -4.8556e-02, -4.6162e-02,  1.2934e-01,\n",
      "          1.0099e-01, -1.8392e-02,  5.2972e-03,  4.7909e-02,  1.9276e-01,\n",
      "          1.1588e-01, -1.1279e-01, -1.1455e-01,  7.0715e-02, -2.2617e-02,\n",
      "         -1.4345e-01, -2.1642e-01,  3.2030e-02, -6.5298e-02, -1.3010e-01],\n",
      "        [ 4.2063e-02,  2.1774e-01,  1.8572e-01,  1.2749e-01, -1.6864e-01,\n",
      "          1.2762e-01,  1.1565e-01,  7.4605e-02, -5.5701e-02, -3.8937e-03,\n",
      "         -1.2128e-01, -5.0369e-02, -3.6809e-02, -3.1675e-02, -2.2253e-01,\n",
      "          7.8886e-02, -1.8295e-01, -1.6722e-01,  9.0867e-02, -1.8304e-01],\n",
      "        [ 2.1332e-01,  2.1411e-01, -7.1058e-02, -3.8537e-02, -1.7107e-01,\n",
      "          1.3824e-02,  4.0883e-02, -1.2266e-01,  7.0119e-02, -8.6055e-03,\n",
      "         -6.5915e-02, -7.7361e-04, -2.1675e-01, -1.5752e-01,  2.9598e-02,\n",
      "          2.1594e-01, -1.3254e-01,  1.4335e-01,  2.0464e-02, -9.6203e-02],\n",
      "        [ 2.4499e-04,  1.0720e-01,  2.6603e-02,  1.9794e-01, -4.2319e-02,\n",
      "         -1.4932e-01,  2.0206e-01, -6.3862e-02,  1.4188e-01,  1.4749e-01,\n",
      "          3.9602e-02, -1.8352e-01, -2.1952e-01,  1.0615e-01,  5.4670e-02,\n",
      "          1.3011e-01, -1.9709e-01, -3.0807e-02,  1.9320e-01,  1.1748e-01],\n",
      "        [ 1.8582e-01,  1.7556e-01, -5.4866e-02, -1.5227e-01, -1.3333e-01,\n",
      "          7.5943e-02, -8.8758e-02,  8.8681e-04,  3.5684e-02,  1.3881e-01,\n",
      "          1.2471e-01,  1.2762e-01,  9.0880e-03,  1.2624e-01, -1.0468e-01,\n",
      "         -8.9498e-02, -3.5404e-02, -1.2985e-01,  1.9946e-01, -3.4264e-02],\n",
      "        [ 6.5008e-02, -6.9521e-03, -3.0583e-03, -1.1918e-01, -1.6417e-02,\n",
      "         -2.0002e-01, -1.0693e-01,  1.7208e-01, -2.0973e-01,  3.6797e-02,\n",
      "          3.5623e-02, -2.1402e-01,  4.3674e-02,  1.0382e-01,  1.6195e-01,\n",
      "          4.1854e-02,  1.7479e-01,  8.4166e-03,  8.5199e-02,  6.6011e-02],\n",
      "        [-1.6487e-01, -4.9085e-02, -1.5747e-01,  1.7374e-01,  1.0667e-01,\n",
      "         -9.4833e-02, -4.0919e-02,  5.9693e-02, -1.9497e-01,  6.3140e-03,\n",
      "          1.0666e-01,  2.8018e-02, -2.0574e-02,  1.4376e-01,  9.7266e-02,\n",
      "         -1.9879e-01,  1.3160e-01, -1.7636e-01,  1.2561e-01,  1.8829e-01],\n",
      "        [ 2.0608e-02, -7.5573e-02, -1.1550e-01, -9.9290e-02, -4.1218e-02,\n",
      "         -9.2703e-02,  2.2198e-01, -9.6452e-02,  9.2839e-02, -2.0736e-01,\n",
      "          1.1334e-01,  1.9027e-01, -1.6030e-01, -5.7052e-02,  5.3294e-02,\n",
      "         -2.1404e-01, -1.4148e-01,  7.7767e-02,  2.1056e-01, -6.6801e-02],\n",
      "        [-2.0364e-03, -2.1586e-01, -2.1463e-01,  1.9447e-01,  1.8975e-01,\n",
      "         -6.8148e-02, -8.8418e-02, -1.4475e-02,  1.0375e-01, -2.1609e-01,\n",
      "         -1.2386e-01, -7.3717e-02, -9.7327e-02, -5.9071e-03,  1.6310e-01,\n",
      "          4.8822e-02, -2.0236e-01,  2.0681e-01,  1.8005e-01,  7.0404e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1095,  0.0836,  0.1833, -0.2151, -0.1392, -0.0284, -0.0425, -0.1485,\n",
      "        -0.2144, -0.1708, -0.0347,  0.1898,  0.1482,  0.0645, -0.1373, -0.0900,\n",
      "         0.0215, -0.2073, -0.0102,  0.0455,  0.2022, -0.0587,  0.2195,  0.0169,\n",
      "        -0.1551,  0.2113,  0.1939, -0.1767, -0.1297,  0.1069],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1123,  0.1241, -0.0871,  ...,  0.1139,  0.0594, -0.0519],\n",
      "        [ 0.0592,  0.1546,  0.1343,  ..., -0.1459,  0.0904,  0.0278],\n",
      "        [ 0.0019, -0.0117,  0.1542,  ...,  0.1597,  0.1546, -0.1344],\n",
      "        ...,\n",
      "        [-0.1407, -0.1203, -0.1250,  ...,  0.0393,  0.1626, -0.0863],\n",
      "        [ 0.0852, -0.0249, -0.1033,  ...,  0.0305, -0.1305,  0.0260],\n",
      "        [ 0.1099,  0.1278, -0.0302,  ..., -0.0994,  0.0479,  0.0188]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0129, -0.0992,  0.1278,  0.0262,  0.0678,  0.0213, -0.0908, -0.0686,\n",
      "         0.1215, -0.1595, -0.0631, -0.1671, -0.1091,  0.0245, -0.0416,  0.0608,\n",
      "         0.1391,  0.0384,  0.1734, -0.0005,  0.1761,  0.0295, -0.0047,  0.0510,\n",
      "        -0.1440,  0.1489,  0.1671, -0.1606, -0.1087,  0.1443,  0.0459, -0.0660,\n",
      "        -0.0910, -0.0418, -0.0344,  0.1684, -0.1110,  0.0257,  0.0517, -0.0906],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0099, -0.0286,  0.1206,  0.1131, -0.0076, -0.0810, -0.0908, -0.0673,\n",
      "          0.1372,  0.1285,  0.0314, -0.1473,  0.0505, -0.0294,  0.0884,  0.0822,\n",
      "          0.1326,  0.0115,  0.0121,  0.0011,  0.1092,  0.1194,  0.0745, -0.0316,\n",
      "         -0.0459,  0.1465, -0.0818,  0.0272,  0.0497,  0.0819, -0.0959, -0.0305,\n",
      "         -0.0046, -0.0144, -0.1357,  0.0682, -0.1209, -0.0399,  0.0485,  0.1243],\n",
      "        [-0.0275, -0.0875, -0.0705, -0.0015, -0.0864,  0.0391,  0.0715,  0.1492,\n",
      "          0.1330, -0.0323,  0.0672,  0.0207,  0.0750, -0.0719,  0.1485,  0.0011,\n",
      "          0.1001,  0.1173, -0.0335,  0.1212, -0.0409,  0.0787, -0.0245, -0.1250,\n",
      "         -0.0403,  0.0600,  0.1263, -0.1290, -0.0918,  0.0824, -0.0808,  0.0082,\n",
      "         -0.1103,  0.0100, -0.0765,  0.0353,  0.0623,  0.0303, -0.1243,  0.1185],\n",
      "        [ 0.1331, -0.1418,  0.1295, -0.1363,  0.0743, -0.0093, -0.0126, -0.0770,\n",
      "          0.1245,  0.0256, -0.0110, -0.0056,  0.1146,  0.1567, -0.0901, -0.1402,\n",
      "         -0.1070, -0.0858, -0.1292, -0.0571, -0.0612, -0.0011,  0.0451,  0.1016,\n",
      "         -0.1043, -0.1133, -0.0689, -0.1165,  0.0085, -0.1199, -0.0073, -0.0216,\n",
      "         -0.1224, -0.0714,  0.0691, -0.0453, -0.1396,  0.1286, -0.0493,  0.0191]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1272,  0.0787, -0.1278], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Example of parameters() in models\n",
    "# iterator(모든 파라미터) 출력\n",
    "param_iterator = model.parameters()   \n",
    "\n",
    "# 위의 network에 있는 모든 파라미터를 for문을 통해서 출력함.\n",
    "for param in param_iterator:\n",
    "    print(param)\n",
    "\n",
    "    \n",
    "\n",
    "# 첫번째 layer의 W\n",
    "# 첫번째 layer의 bias\n",
    "# 두번째 layer의 W\n",
    "# 두번째 layer의 bias\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d17bb0",
   "metadata": {},
   "source": [
    "## nn.Sequential() example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "929f4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential() example\n",
    "\n",
    "class FCN_seq(nn.Module):\n",
    "    def __init__(self):     # 초기화 함수 정의\n",
    "        super().__init__()  # super의 __init__ 실행\n",
    "        \n",
    "        # 순차적으로 내가 실행할 모듈들을 정의한다.\n",
    "        # Sequential 함수는 forward함수가 내부적으로 정의되어 있다.\n",
    "        self.fc = nn.Sequential(nn.Linear(20, 30),\n",
    "                      nn.ReLU(True),\n",
    "                      nn.Linear(30, 40),\n",
    "                      nn.ReLU(True),\n",
    "                      nn.Linear(40, 3)   # 마지막 layer는 Linear layer로\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "# self.lin1 = nn.Linear(20, 30)\n",
    "# self.lin2 = nn.Linear(30, 40)\n",
    "# self.lin3 = nn.Linear(40, 3)\n",
    "# self.relu = nn.ReLU(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "653f0bb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 20])\n",
      "FCN_seq(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=30, out_features=40, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=40, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[ 0.1613,  0.0474,  0.1423],\n",
      "        [ 0.1216,  0.0780,  0.0733],\n",
      "        [ 0.1969,  0.1477,  0.0351],\n",
      "        [ 0.1259,  0.0780,  0.1492],\n",
      "        [ 0.0796,  0.1557, -0.0115],\n",
      "        [ 0.1025,  0.0723,  0.1972],\n",
      "        [ 0.1049,  0.2304, -0.0196],\n",
      "        [ 0.2009,  0.1772,  0.0686],\n",
      "        [ 0.0065,  0.3023,  0.1493],\n",
      "        [ 0.2442, -0.0287,  0.2737],\n",
      "        [ 0.2066,  0.1528,  0.0567],\n",
      "        [ 0.2014,  0.2286,  0.1846],\n",
      "        [ 0.2565,  0.0261,  0.0793],\n",
      "        [ 0.2581,  0.1663,  0.1000],\n",
      "        [ 0.2335,  0.3101,  0.0227],\n",
      "        [ 0.1506,  0.0258,  0.1484],\n",
      "        [ 0.1547,  0.1805, -0.0363],\n",
      "        [ 0.1707,  0.2219, -0.0271],\n",
      "        [ 0.2267,  0.1917,  0.0362],\n",
      "        [ 0.1817,  0.1746,  0.0060],\n",
      "        [ 0.1650,  0.0331,  0.1363],\n",
      "        [ 0.0883,  0.1512,  0.0506],\n",
      "        [ 0.1654,  0.0886,  0.2320],\n",
      "        [ 0.2168,  0.0607,  0.1174],\n",
      "        [ 0.0745,  0.2253,  0.0383],\n",
      "        [ 0.3141,  0.1315,  0.1308],\n",
      "        [ 0.1493,  0.0607,  0.1488],\n",
      "        [ 0.1458,  0.1550, -0.0270],\n",
      "        [ 0.2477,  0.1821,  0.0342],\n",
      "        [ 0.1896,  0.2815, -0.0078],\n",
      "        [ 0.1405,  0.0122,  0.2558],\n",
      "        [ 0.1551,  0.1539, -0.0020],\n",
      "        [ 0.0595,  0.0909,  0.1541],\n",
      "        [ 0.1129,  0.0855,  0.2236],\n",
      "        [ 0.1268,  0.3080,  0.0409],\n",
      "        [ 0.0728,  0.2503,  0.1135],\n",
      "        [ 0.1436,  0.2091,  0.0691],\n",
      "        [ 0.1750, -0.0101,  0.1224],\n",
      "        [ 0.0279, -0.0216,  0.1104],\n",
      "        [ 0.1095,  0.1664, -0.1194],\n",
      "        [ 0.0748,  0.1219,  0.0871],\n",
      "        [-0.0367,  0.0451,  0.2273],\n",
      "        [ 0.1576,  0.1298,  0.0901],\n",
      "        [ 0.1459,  0.1696,  0.0473],\n",
      "        [ 0.0922,  0.2346,  0.0730],\n",
      "        [ 0.0842,  0.0173,  0.2200],\n",
      "        [ 0.2710,  0.0649,  0.2377],\n",
      "        [ 0.0995,  0.1327,  0.0879],\n",
      "        [ 0.0733,  0.2322,  0.1033],\n",
      "        [ 0.1515,  0.0940, -0.0213],\n",
      "        [ 0.1248,  0.1274,  0.0092],\n",
      "        [ 0.2959,  0.1230,  0.1235],\n",
      "        [ 0.1534,  0.1846, -0.0096],\n",
      "        [ 0.0742,  0.2431, -0.0146],\n",
      "        [ 0.1574,  0.0040,  0.1750],\n",
      "        [ 0.1238,  0.1614,  0.0361],\n",
      "        [ 0.0994,  0.0473,  0.0531],\n",
      "        [ 0.1075,  0.0929, -0.1497],\n",
      "        [ 0.2958, -0.0499,  0.1467],\n",
      "        [ 0.1481,  0.1489,  0.0972]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_seq = FCN_seq()  # 인스턴스 생성\n",
    "print(Xtrain.shape)\n",
    "print(model_seq)\n",
    "print(model_seq(Xtrain))\n",
    "# print(model_seq.fc(Xtrain))  -> forward함수 정의 안 했을 때, fc를 붙인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed29efaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 20])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0db292b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=20, out_features=30, bias=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seq.fc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16fa5b",
   "metadata": {},
   "source": [
    "### 다른 NN 방법(class 사용x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2796ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 또는\n",
    "model_by_sequential = nn.Sequential(nn.Linear(20, 30),\n",
    "                      nn.ReLU(True),\n",
    "                      nn.Linear(30, 40),\n",
    "                      nn.ReLU(True),\n",
    "                      nn.Linear(40, 3)   # 마지막 layer는 Linear layer로\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bc7e162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1290,  0.1012, -0.0432],\n",
       "        [-0.1807,  0.0422, -0.0590],\n",
       "        [-0.0867,  0.0434, -0.0832],\n",
       "        [-0.1133,  0.1099, -0.0820],\n",
       "        [-0.1334,  0.0641, -0.1664],\n",
       "        [-0.2149,  0.0981, -0.1222],\n",
       "        [-0.0897,  0.0634, -0.0132],\n",
       "        [-0.0316,  0.1087, -0.0741],\n",
       "        [-0.1536,  0.1385, -0.1129],\n",
       "        [-0.0053,  0.0458, -0.1444],\n",
       "        [-0.1036,  0.0451, -0.0375],\n",
       "        [-0.0530,  0.2289, -0.0727],\n",
       "        [-0.0994,  0.1190, -0.0743],\n",
       "        [-0.0525,  0.1025, -0.0331],\n",
       "        [-0.0651, -0.0479, -0.1240],\n",
       "        [-0.0760,  0.0858, -0.0104],\n",
       "        [-0.0589,  0.1708, -0.1553],\n",
       "        [-0.1079,  0.0682,  0.0143],\n",
       "        [-0.0809,  0.0537, -0.0853],\n",
       "        [-0.1456,  0.0114, -0.0614],\n",
       "        [-0.0732,  0.0301, -0.0336],\n",
       "        [-0.1034,  0.0222, -0.1068],\n",
       "        [-0.0352,  0.0648, -0.0863],\n",
       "        [-0.0185,  0.0939, -0.1200],\n",
       "        [-0.1203,  0.0337,  0.0012],\n",
       "        [-0.0015,  0.1213, -0.0419],\n",
       "        [-0.0330,  0.1190, -0.0865],\n",
       "        [-0.0005,  0.0610, -0.0253],\n",
       "        [-0.0919,  0.1153,  0.0097],\n",
       "        [-0.0367,  0.0423, -0.0108],\n",
       "        [-0.0278,  0.1138, -0.0366],\n",
       "        [-0.0742,  0.1300, -0.0760],\n",
       "        [-0.1242,  0.0762, -0.0647],\n",
       "        [-0.0658,  0.0426, -0.0094],\n",
       "        [-0.0856,  0.0091, -0.1135],\n",
       "        [-0.1034,  0.0892, -0.0680],\n",
       "        [-0.0994,  0.1092, -0.0953],\n",
       "        [-0.0392,  0.0796, -0.1056],\n",
       "        [-0.1600,  0.0590, -0.0366],\n",
       "        [-0.0855,  0.0595, -0.0094],\n",
       "        [-0.0960,  0.0561, -0.0675],\n",
       "        [-0.0710,  0.0747, -0.0750],\n",
       "        [-0.0310,  0.1639, -0.1078],\n",
       "        [-0.1395,  0.0787, -0.0904],\n",
       "        [-0.0276,  0.0048, -0.0068],\n",
       "        [-0.0805,  0.1598, -0.0854],\n",
       "        [-0.0592,  0.0072, -0.1888],\n",
       "        [-0.0234,  0.1326, -0.1984],\n",
       "        [-0.0774,  0.0791, -0.0556],\n",
       "        [-0.1249,  0.1649, -0.0123],\n",
       "        [-0.1809, -0.0214, -0.0110],\n",
       "        [-0.1319,  0.1065, -0.1589],\n",
       "        [-0.1778,  0.0543, -0.0465],\n",
       "        [-0.1090,  0.0198, -0.0476],\n",
       "        [-0.0900,  0.0762,  0.0130],\n",
       "        [-0.1184,  0.1160, -0.0723],\n",
       "        [-0.1005,  0.0980, -0.1101],\n",
       "        [-0.0190,  0.0927, -0.0351],\n",
       "        [-0.1039,  0.0777, -0.0478],\n",
       "        [ 0.0297,  0.0936, -0.0460]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_by_sequential(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c125628d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a16dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_seq_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        # self.fcn_block(20, 30)가 두 개의 list를 리턴 + 리스트에 리스트를 더해서 아이템들을 추가\n",
    "        temp = self.fcn_block(20, 30)+self.fcn_block(30, 40)+[nn.Linear(40,1)]   # 마지막은 linear layer로 예외처리 해줌\n",
    "        self.fc = nn.Sequential(*temp)  # 리스트를 풀어서 안에 있는 것을 나열한다.\n",
    "        \n",
    "        \n",
    "    def fcn_block(self, in_dim, out_dim):\n",
    "        return [nn.Linear(in_dim, out_dim),  # nn.Sequential을 하지 않고 두개의 list를 return한다.\n",
    "                             nn.ReLU(True)]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "# self.lin1 = nn.Linear(20, 30)\n",
    "# self.lin2 = nn.Linear(30, 40)\n",
    "# self.lin3 = nn.Linear(40, 3)\n",
    "# self.relu = nn.ReLU(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac30117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCN_seq_v2(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=30, out_features=40, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=40, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[ 0.0024],\n",
      "        [-0.0164],\n",
      "        [ 0.0110],\n",
      "        [ 0.0904],\n",
      "        [ 0.0215],\n",
      "        [ 0.0451],\n",
      "        [ 0.1046],\n",
      "        [-0.0817],\n",
      "        [-0.0119],\n",
      "        [-0.0311],\n",
      "        [ 0.0728],\n",
      "        [-0.0408],\n",
      "        [ 0.0488],\n",
      "        [ 0.0549],\n",
      "        [ 0.0904],\n",
      "        [ 0.0783],\n",
      "        [-0.0463],\n",
      "        [ 0.0951],\n",
      "        [-0.0649],\n",
      "        [-0.0142],\n",
      "        [ 0.0607],\n",
      "        [ 0.0439],\n",
      "        [-0.0259],\n",
      "        [-0.0705],\n",
      "        [ 0.0784],\n",
      "        [ 0.1228],\n",
      "        [-0.0202],\n",
      "        [ 0.1298],\n",
      "        [ 0.0849],\n",
      "        [ 0.0272],\n",
      "        [-0.0376],\n",
      "        [ 0.0315],\n",
      "        [ 0.0046],\n",
      "        [ 0.0089],\n",
      "        [ 0.0573],\n",
      "        [ 0.0502],\n",
      "        [ 0.0856],\n",
      "        [ 0.0536],\n",
      "        [-0.0321],\n",
      "        [ 0.0411],\n",
      "        [-0.0041],\n",
      "        [-0.0203],\n",
      "        [ 0.0841],\n",
      "        [ 0.0357],\n",
      "        [ 0.0516],\n",
      "        [ 0.0837],\n",
      "        [-0.0443],\n",
      "        [-0.0733],\n",
      "        [ 0.0838],\n",
      "        [ 0.0228],\n",
      "        [ 0.0315],\n",
      "        [-0.0350],\n",
      "        [ 0.0234],\n",
      "        [-0.0429],\n",
      "        [ 0.0295],\n",
      "        [ 0.0258],\n",
      "        [-0.0257],\n",
      "        [ 0.1848],\n",
      "        [ 0.0971],\n",
      "        [ 0.0309]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_seq_v2 = FCN_seq_v2()  # 인스턴스 생성\n",
    "print(model_seq_v2)\n",
    "print(model_seq_v2(Xtrain))  # forward함수가 잘 동작하는지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42e688",
   "metadata": {},
   "source": [
    "### zip함수\n",
    "각 리스트마다 순차적으로 쌍으로 묶어준다. (같은 인덱스끼리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "122571ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 30  j= 20\n",
      "i= 40  j= 80\n",
      "i= 50  j= 99\n",
      "i= 60  j= 100\n"
     ]
    }
   ],
   "source": [
    "hlayer1 = [30, 40, 50, 60]\n",
    "hlayer2 = [20, 80, 99, 100]\n",
    "\n",
    "for i, j in zip(hlayer1, hlayer2):\n",
    "    print('i=', i, ' j=', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d606f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 30  j= 60\n",
      "i= 40  j= 80\n",
      "i= 50  j= 90\n"
     ]
    }
   ],
   "source": [
    "# hlayer0 = [30, 40, 50, 60, 80, 90]\n",
    "\n",
    "# for i, j in zip(hlayer0[:3], hlayer0[3:]):\n",
    "#     print('i=', i, ' j=', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "514cccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_final(nn.Module):\n",
    "    def __init__(self, in_dim, hlayer, out_dim):   # hidden layer의 수를 list로 받는다 -> 일반적인 list를 준 것에 대해서 network를 만든다\n",
    "        super().__init__()\n",
    "        \n",
    "        l_list = self.fcn_block(in_dim, hlayer[0])  # 첫번째 layer는 따로 만들어줌\n",
    "        \n",
    "        for l1, l2 in zip(hlayer[:-1], hlayer[1:]):  # zip(마지막것만 빼라, 첫번째것만 빼라)\n",
    "            l_list = l_list + self.fcn_block(l1, l2)\n",
    "        \n",
    "        l_list = l_list + [nn.Linear(hlayer[-1], out_dim)]  # 마지막 layer는 예외처리로 직접 작성\n",
    "                                                            # Linear는 list가 아니니까 []를 씌워주고 입력\n",
    "        \n",
    "        self.fc = nn.Sequential(*l_list)\n",
    "        \n",
    "        \n",
    "    def fcn_block(self, in_dim, out_dim):\n",
    "        return [nn.Linear(in_dim, out_dim),\n",
    "                             nn.ReLU(True)]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05a543cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlayer = [30, 40]\n",
    "in_dim = 20\n",
    "out_dim= 3\n",
    "\n",
    "myfcn_final = FCN_final(in_dim, hlayer, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d545559c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN_final(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=30, out_features=40, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=40, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfcn_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7c5eb",
   "metadata": {},
   "source": [
    "## Ordered dict example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932eeab1",
   "metadata": {},
   "source": [
    "**<이름으로 부르면 좋은점>**   \n",
    "1. 사람들한테 설명하는 데 편리하다.\n",
    "2. 어떤 이름에 있는 것들만 교체하는거 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d419c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordered dict example\n",
    "# nn.Sequential() example\n",
    "\n",
    "class FCN_seq_ordered_dic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(OrderedDict([('lin1', nn.Linear(20, 30)),  # key와 value 만들기 위해 튜플로 작성\n",
    "                      ('relu1', nn.ReLU(True)),\n",
    "                      ('lin2', nn.Linear(30, 40)),\n",
    "                      ('relu2',nn.ReLU(True)),\n",
    "                      ('lin3', nn.Linear(40, 3))\n",
    "                                            ])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "# self.lin1 = nn.Linear(20, 30)\n",
    "# self.lin2 = nn.Linear(30, 40)\n",
    "# self.lin3 = nn.Linear(40, 3)\n",
    "# self.relu = nn.ReLU(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a876e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleList(), ModuleDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a3f5d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN_seq_ordered_dic(\n",
       "  (fc): Sequential(\n",
       "    (lin1): Linear(in_features=20, out_features=30, bias=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (lin2): Linear(in_features=30, out_features=40, bias=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (lin3): Linear(in_features=40, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_FCN_seq_ordered_dic = FCN_seq_ordered_dic()\n",
    "model_FCN_seq_ordered_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7a3ec5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=20, out_features=30, bias=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_FCN_seq_ordered_dic.fc.lin1  # lin1처럼 이름으로 부를 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53780fb0",
   "metadata": {},
   "source": [
    "모델 학습을 다 끝내놓고 network를 저장할 때 `model.state_dict()`를 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93369a92",
   "metadata": {},
   "source": [
    "### state_dict() \n",
    "모델을 다 설명해주는 것.(layer별 이름, 모듈들의 이름, 파라미터, bias...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb4c2ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lin1.weight',\n",
       "              tensor([[ 1.1542e-01, -1.5343e-01, -1.4473e-01,  6.0137e-02,  3.4798e-02,\n",
       "                       -1.9658e-01, -1.6290e-01, -1.7801e-01,  1.7759e-01,  1.4619e-01,\n",
       "                       -1.1770e-01,  3.3738e-02, -5.1431e-02,  1.9439e-01, -2.0253e-01,\n",
       "                       -1.6313e-01, -1.8617e-01,  1.3013e-01, -1.1178e-01,  1.3318e-01],\n",
       "                      [ 7.5586e-02, -1.0930e-01, -6.0237e-02, -9.0490e-02,  9.8650e-02,\n",
       "                       -8.9674e-02,  7.5885e-02,  1.5974e-02, -1.7857e-01, -1.4056e-01,\n",
       "                       -1.5504e-01,  1.1717e-01,  4.8451e-02, -5.6224e-03, -1.3314e-01,\n",
       "                       -2.2155e-01, -7.9131e-02, -1.7114e-01, -2.2241e-01, -2.1327e-01],\n",
       "                      [-1.4606e-01, -2.1738e-01,  6.9704e-02,  8.3668e-02,  9.0129e-02,\n",
       "                       -2.2260e-01,  1.3827e-01,  4.3595e-02,  1.1434e-01, -5.4957e-02,\n",
       "                        1.0738e-01, -2.1207e-01,  7.6162e-02,  1.7731e-01,  9.3997e-02,\n",
       "                       -2.1077e-01, -6.2772e-02, -2.7768e-02,  1.1716e-01, -8.4613e-02],\n",
       "                      [ 3.2161e-02, -7.8497e-02, -3.5522e-02, -4.4512e-03, -1.2840e-01,\n",
       "                       -1.5520e-01,  1.8460e-01,  1.8787e-01, -6.7696e-03,  1.9355e-01,\n",
       "                        1.7231e-01, -1.4085e-01, -8.8852e-02, -1.5358e-01,  7.2294e-02,\n",
       "                        5.6551e-03, -3.0665e-02,  1.6602e-01, -8.7401e-02, -1.2238e-01],\n",
       "                      [-1.5896e-01, -1.1376e-01,  2.1552e-01, -1.0872e-01, -6.1526e-02,\n",
       "                        1.0078e-01, -1.7174e-01, -1.3572e-01,  1.6458e-01, -8.5763e-02,\n",
       "                       -1.1603e-01, -2.8930e-02, -3.1862e-02,  4.0865e-02,  8.1979e-02,\n",
       "                       -1.7241e-01,  2.0321e-01, -2.1259e-01,  7.5732e-02, -6.3092e-02],\n",
       "                      [-9.4428e-02,  2.0917e-01, -2.1548e-01,  1.5271e-02,  1.4328e-01,\n",
       "                        1.9990e-01, -1.8904e-01,  2.6941e-02,  2.2123e-01, -2.1903e-01,\n",
       "                        5.4616e-02, -1.4011e-01,  3.6300e-02, -2.0227e-01,  1.1002e-01,\n",
       "                       -4.9605e-02,  1.3364e-01, -3.3625e-03, -3.1873e-02, -5.4288e-02],\n",
       "                      [-2.0474e-01, -9.4979e-02,  7.4261e-03, -1.7299e-01,  1.9624e-01,\n",
       "                       -1.4543e-01, -8.7115e-02, -2.9903e-02, -1.8114e-01, -1.7667e-03,\n",
       "                       -1.0768e-01, -1.8717e-01,  5.4847e-03,  1.9796e-01,  5.5063e-02,\n",
       "                        1.8443e-01, -2.1867e-03, -7.8048e-02,  1.8022e-01, -1.1907e-01],\n",
       "                      [-2.0686e-01,  4.8939e-02,  1.1144e-01, -1.3366e-01,  7.8702e-02,\n",
       "                       -7.9332e-02, -2.7131e-02, -4.9511e-02, -4.7828e-02, -1.8194e-01,\n",
       "                        5.4802e-02, -5.8818e-02,  1.7098e-01,  3.7323e-02, -1.8287e-01,\n",
       "                       -1.7011e-01, -1.1654e-01,  1.0708e-01, -1.4437e-01, -1.0229e-01],\n",
       "                      [ 4.0596e-02, -1.5503e-01, -1.1011e-01,  2.0276e-01, -1.1822e-01,\n",
       "                       -7.4499e-02,  1.5992e-01,  5.0107e-02,  1.7551e-01, -1.9700e-01,\n",
       "                        1.1177e-01, -3.4202e-02, -1.4325e-01, -9.5770e-02, -2.1629e-01,\n",
       "                        1.5469e-01, -4.2906e-02, -2.1926e-01,  1.9123e-01,  1.4483e-01],\n",
       "                      [ 9.2458e-02,  2.1885e-01,  2.0193e-01,  2.0897e-01,  2.0025e-01,\n",
       "                        1.8172e-01,  5.8538e-02, -1.8726e-02,  8.5036e-03, -9.2926e-02,\n",
       "                        1.0506e-01,  6.4780e-03,  5.2755e-02, -1.4404e-01, -8.4194e-02,\n",
       "                        1.6764e-01,  2.1748e-02,  7.7165e-02, -1.9521e-01, -9.5754e-02],\n",
       "                      [-5.6909e-03, -2.3464e-02,  1.4274e-01, -6.7481e-02,  1.8662e-01,\n",
       "                       -4.3247e-02,  8.6972e-02, -1.5743e-01,  3.7954e-02, -2.1800e-01,\n",
       "                       -1.9185e-01, -1.4311e-01,  6.2600e-02,  1.7331e-02,  2.1642e-02,\n",
       "                       -2.4460e-02,  1.8789e-01,  2.1905e-01,  9.4963e-02,  1.7782e-01],\n",
       "                      [-1.1051e-02, -8.2205e-02,  4.3471e-02,  1.4703e-01, -3.4559e-02,\n",
       "                       -1.7169e-01,  7.3843e-02, -5.7001e-02, -1.0764e-01,  1.4124e-01,\n",
       "                       -7.5897e-03, -1.2782e-01, -1.3380e-01,  2.1058e-01,  1.4544e-01,\n",
       "                       -1.6034e-01,  1.0689e-04, -4.8377e-03,  1.0659e-01, -1.9151e-01],\n",
       "                      [ 3.3233e-03,  7.4504e-02,  1.1379e-01,  2.3381e-02, -1.1084e-01,\n",
       "                       -2.1467e-01,  1.2086e-01,  2.0414e-01, -6.7511e-02, -1.9983e-01,\n",
       "                        6.5424e-02, -1.5677e-01, -1.4610e-01,  6.0952e-02, -2.1234e-02,\n",
       "                       -1.5210e-02,  2.0056e-01, -3.9221e-02,  1.3611e-01, -6.9592e-02],\n",
       "                      [ 1.6639e-01,  6.8216e-02,  1.9926e-01,  7.7989e-02, -7.9205e-02,\n",
       "                        9.7460e-02,  1.9051e-01,  4.2381e-02,  1.0309e-01, -1.5731e-01,\n",
       "                        1.2116e-01, -2.1137e-01,  1.9767e-01,  9.2355e-02,  1.4715e-02,\n",
       "                       -2.0929e-01,  2.0334e-01,  1.4258e-01,  2.0614e-01,  8.7902e-02],\n",
       "                      [ 7.5306e-02, -5.7922e-02, -9.1634e-02,  8.8638e-02,  7.7548e-02,\n",
       "                       -7.2493e-03,  3.5567e-02,  7.6812e-02, -2.1608e-01, -1.9775e-01,\n",
       "                       -5.9525e-02,  1.5229e-01,  1.0909e-01, -2.1342e-01, -6.7138e-02,\n",
       "                       -2.3636e-02,  7.3442e-02,  1.8294e-01,  4.8906e-03, -1.7405e-01],\n",
       "                      [ 1.3720e-02, -8.6348e-02, -1.3666e-01,  1.2385e-01, -3.0042e-02,\n",
       "                        1.0566e-01,  1.7411e-01,  2.1976e-02,  1.2595e-01, -1.0561e-01,\n",
       "                        1.2482e-01,  8.9331e-03,  1.4859e-01,  2.1378e-01, -1.6915e-01,\n",
       "                        2.1227e-01,  1.7069e-01, -1.8013e-01,  6.5548e-02, -1.8204e-01],\n",
       "                      [ 7.2535e-02,  3.4579e-02, -2.1346e-01,  8.1173e-02, -6.1420e-02,\n",
       "                        2.2289e-01,  2.1824e-01,  7.7191e-02,  1.7626e-01,  8.0762e-02,\n",
       "                        2.1155e-01, -2.0645e-02,  5.5265e-02, -3.6936e-02, -3.9105e-03,\n",
       "                       -6.7455e-02, -8.8493e-02,  1.2935e-01,  9.5956e-02, -9.8709e-02],\n",
       "                      [-1.9707e-01,  1.0933e-01, -2.0569e-01, -9.8949e-03, -1.9240e-01,\n",
       "                        1.8723e-01,  1.8208e-01, -1.8377e-01, -2.0789e-01, -1.3221e-01,\n",
       "                        6.1702e-02, -1.1182e-01, -5.3251e-02, -5.6680e-02,  4.1017e-02,\n",
       "                       -1.0577e-01,  1.0170e-01, -1.6001e-01, -1.9904e-02,  1.8743e-01],\n",
       "                      [-1.2347e-01,  1.7156e-01,  1.1638e-01, -7.2229e-02,  7.7539e-02,\n",
       "                       -1.5106e-01,  1.7996e-02,  6.3339e-02,  1.9341e-01, -1.2513e-01,\n",
       "                       -1.2717e-01, -1.1160e-01, -2.1783e-01,  4.5219e-02, -1.8509e-01,\n",
       "                       -6.0026e-02, -9.9769e-02,  1.2502e-02,  6.7831e-02, -1.6249e-01],\n",
       "                      [-6.6012e-02, -2.1497e-02, -1.2926e-01,  5.1656e-02,  8.2778e-02,\n",
       "                       -1.5936e-01,  1.7583e-01,  1.5889e-01, -6.7177e-02,  3.3271e-02,\n",
       "                        2.7226e-02, -3.0851e-02, -1.5007e-01,  1.8426e-01,  1.1004e-01,\n",
       "                       -1.8686e-01, -1.7330e-01,  2.1264e-01,  3.5689e-02, -5.1373e-02],\n",
       "                      [ 1.5155e-01,  6.4238e-02,  8.0268e-02,  1.7740e-01, -1.7452e-02,\n",
       "                        2.1307e-01, -1.5707e-01, -1.3606e-01,  3.7888e-02, -2.7147e-02,\n",
       "                        1.2760e-01, -6.0805e-02,  3.5162e-02, -4.6463e-02,  1.6787e-01,\n",
       "                       -1.4667e-02,  3.4322e-03,  1.6381e-01,  2.1739e-01,  2.1396e-01],\n",
       "                      [-2.1863e-01, -1.2635e-01, -4.8556e-02, -4.6162e-02,  1.2934e-01,\n",
       "                        1.0099e-01, -1.8392e-02,  5.2972e-03,  4.7909e-02,  1.9276e-01,\n",
       "                        1.1588e-01, -1.1279e-01, -1.1455e-01,  7.0715e-02, -2.2617e-02,\n",
       "                       -1.4345e-01, -2.1642e-01,  3.2030e-02, -6.5298e-02, -1.3010e-01],\n",
       "                      [ 4.2063e-02,  2.1774e-01,  1.8572e-01,  1.2749e-01, -1.6864e-01,\n",
       "                        1.2762e-01,  1.1565e-01,  7.4605e-02, -5.5701e-02, -3.8937e-03,\n",
       "                       -1.2128e-01, -5.0369e-02, -3.6809e-02, -3.1675e-02, -2.2253e-01,\n",
       "                        7.8886e-02, -1.8295e-01, -1.6722e-01,  9.0867e-02, -1.8304e-01],\n",
       "                      [ 2.1332e-01,  2.1411e-01, -7.1058e-02, -3.8537e-02, -1.7107e-01,\n",
       "                        1.3824e-02,  4.0883e-02, -1.2266e-01,  7.0119e-02, -8.6055e-03,\n",
       "                       -6.5915e-02, -7.7361e-04, -2.1675e-01, -1.5752e-01,  2.9598e-02,\n",
       "                        2.1594e-01, -1.3254e-01,  1.4335e-01,  2.0464e-02, -9.6203e-02],\n",
       "                      [ 2.4499e-04,  1.0720e-01,  2.6603e-02,  1.9794e-01, -4.2319e-02,\n",
       "                       -1.4932e-01,  2.0206e-01, -6.3862e-02,  1.4188e-01,  1.4749e-01,\n",
       "                        3.9602e-02, -1.8352e-01, -2.1952e-01,  1.0615e-01,  5.4670e-02,\n",
       "                        1.3011e-01, -1.9709e-01, -3.0807e-02,  1.9320e-01,  1.1748e-01],\n",
       "                      [ 1.8582e-01,  1.7556e-01, -5.4866e-02, -1.5227e-01, -1.3333e-01,\n",
       "                        7.5943e-02, -8.8758e-02,  8.8681e-04,  3.5684e-02,  1.3881e-01,\n",
       "                        1.2471e-01,  1.2762e-01,  9.0880e-03,  1.2624e-01, -1.0468e-01,\n",
       "                       -8.9498e-02, -3.5404e-02, -1.2985e-01,  1.9946e-01, -3.4264e-02],\n",
       "                      [ 6.5008e-02, -6.9521e-03, -3.0583e-03, -1.1918e-01, -1.6417e-02,\n",
       "                       -2.0002e-01, -1.0693e-01,  1.7208e-01, -2.0973e-01,  3.6797e-02,\n",
       "                        3.5623e-02, -2.1402e-01,  4.3674e-02,  1.0382e-01,  1.6195e-01,\n",
       "                        4.1854e-02,  1.7479e-01,  8.4166e-03,  8.5199e-02,  6.6011e-02],\n",
       "                      [-1.6487e-01, -4.9085e-02, -1.5747e-01,  1.7374e-01,  1.0667e-01,\n",
       "                       -9.4833e-02, -4.0919e-02,  5.9693e-02, -1.9497e-01,  6.3140e-03,\n",
       "                        1.0666e-01,  2.8018e-02, -2.0574e-02,  1.4376e-01,  9.7266e-02,\n",
       "                       -1.9879e-01,  1.3160e-01, -1.7636e-01,  1.2561e-01,  1.8829e-01],\n",
       "                      [ 2.0608e-02, -7.5573e-02, -1.1550e-01, -9.9290e-02, -4.1218e-02,\n",
       "                       -9.2703e-02,  2.2198e-01, -9.6452e-02,  9.2839e-02, -2.0736e-01,\n",
       "                        1.1334e-01,  1.9027e-01, -1.6030e-01, -5.7052e-02,  5.3294e-02,\n",
       "                       -2.1404e-01, -1.4148e-01,  7.7767e-02,  2.1056e-01, -6.6801e-02],\n",
       "                      [-2.0364e-03, -2.1586e-01, -2.1463e-01,  1.9447e-01,  1.8975e-01,\n",
       "                       -6.8148e-02, -8.8418e-02, -1.4475e-02,  1.0375e-01, -2.1609e-01,\n",
       "                       -1.2386e-01, -7.3717e-02, -9.7327e-02, -5.9071e-03,  1.6310e-01,\n",
       "                        4.8822e-02, -2.0236e-01,  2.0681e-01,  1.8005e-01,  7.0404e-02]])),\n",
       "             ('lin1.bias',\n",
       "              tensor([ 0.1095,  0.0836,  0.1833, -0.2151, -0.1392, -0.0284, -0.0425, -0.1485,\n",
       "                      -0.2144, -0.1708, -0.0347,  0.1898,  0.1482,  0.0645, -0.1373, -0.0900,\n",
       "                       0.0215, -0.2073, -0.0102,  0.0455,  0.2022, -0.0587,  0.2195,  0.0169,\n",
       "                      -0.1551,  0.2113,  0.1939, -0.1767, -0.1297,  0.1069])),\n",
       "             ('lin2.weight',\n",
       "              tensor([[-0.1123,  0.1241, -0.0871,  ...,  0.1139,  0.0594, -0.0519],\n",
       "                      [ 0.0592,  0.1546,  0.1343,  ..., -0.1459,  0.0904,  0.0278],\n",
       "                      [ 0.0019, -0.0117,  0.1542,  ...,  0.1597,  0.1546, -0.1344],\n",
       "                      ...,\n",
       "                      [-0.1407, -0.1203, -0.1250,  ...,  0.0393,  0.1626, -0.0863],\n",
       "                      [ 0.0852, -0.0249, -0.1033,  ...,  0.0305, -0.1305,  0.0260],\n",
       "                      [ 0.1099,  0.1278, -0.0302,  ..., -0.0994,  0.0479,  0.0188]])),\n",
       "             ('lin2.bias',\n",
       "              tensor([ 0.0129, -0.0992,  0.1278,  0.0262,  0.0678,  0.0213, -0.0908, -0.0686,\n",
       "                       0.1215, -0.1595, -0.0631, -0.1671, -0.1091,  0.0245, -0.0416,  0.0608,\n",
       "                       0.1391,  0.0384,  0.1734, -0.0005,  0.1761,  0.0295, -0.0047,  0.0510,\n",
       "                      -0.1440,  0.1489,  0.1671, -0.1606, -0.1087,  0.1443,  0.0459, -0.0660,\n",
       "                      -0.0910, -0.0418, -0.0344,  0.1684, -0.1110,  0.0257,  0.0517, -0.0906])),\n",
       "             ('lin3.weight',\n",
       "              tensor([[-0.0099, -0.0286,  0.1206,  0.1131, -0.0076, -0.0810, -0.0908, -0.0673,\n",
       "                        0.1372,  0.1285,  0.0314, -0.1473,  0.0505, -0.0294,  0.0884,  0.0822,\n",
       "                        0.1326,  0.0115,  0.0121,  0.0011,  0.1092,  0.1194,  0.0745, -0.0316,\n",
       "                       -0.0459,  0.1465, -0.0818,  0.0272,  0.0497,  0.0819, -0.0959, -0.0305,\n",
       "                       -0.0046, -0.0144, -0.1357,  0.0682, -0.1209, -0.0399,  0.0485,  0.1243],\n",
       "                      [-0.0275, -0.0875, -0.0705, -0.0015, -0.0864,  0.0391,  0.0715,  0.1492,\n",
       "                        0.1330, -0.0323,  0.0672,  0.0207,  0.0750, -0.0719,  0.1485,  0.0011,\n",
       "                        0.1001,  0.1173, -0.0335,  0.1212, -0.0409,  0.0787, -0.0245, -0.1250,\n",
       "                       -0.0403,  0.0600,  0.1263, -0.1290, -0.0918,  0.0824, -0.0808,  0.0082,\n",
       "                       -0.1103,  0.0100, -0.0765,  0.0353,  0.0623,  0.0303, -0.1243,  0.1185],\n",
       "                      [ 0.1331, -0.1418,  0.1295, -0.1363,  0.0743, -0.0093, -0.0126, -0.0770,\n",
       "                        0.1245,  0.0256, -0.0110, -0.0056,  0.1146,  0.1567, -0.0901, -0.1402,\n",
       "                       -0.1070, -0.0858, -0.1292, -0.0571, -0.0612, -0.0011,  0.0451,  0.1016,\n",
       "                       -0.1043, -0.1133, -0.0689, -0.1165,  0.0085, -0.1199, -0.0073, -0.0216,\n",
       "                       -0.1224, -0.0714,  0.0691, -0.0453, -0.1396,  0.1286, -0.0493,  0.0191]])),\n",
       "             ('lin3.bias', tensor([ 0.1272,  0.0787, -0.1278]))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict() example\n",
    "# 모델을 다 설명해주는 것.(layer별 이름, 모듈들의 이름, 파라미터, bias...)\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c586f6",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "<div style=\"text-align:center\"> <img src='https://drive.google.com/uc?export=download&id=1FRhniwGeeutBSJQRdW6GzshMfDrPz7oJ' width=\"250\" height=\"200\"> </div>\n",
    "    \n",
    "Build a Fully connected neural network with\n",
    "\n",
    "- 3 layers\n",
    "- 마지막 layer의 unit 수는 `1` \n",
    "  - 마지막 layer의 activation은 없음 (linear layer)\n",
    "- Data feature 수는 `100` (입력 data의 feature수)\n",
    "\n",
    "- input unit 수는 data 크기를 보고 맞추세요\n",
    "- hidden layer의 unit 수는 `[80, 50]`\n",
    "  - hidden layer의 activation 함수는 ReLU\n",
    "\n",
    "- model class 명 `myFCN`\n",
    "  - instance 명 `my_model` 생성\n",
    "  - `my_model` 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8150a-5d25-40fe-951a-416a5f022112",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "problem setup에서 구성한 neural network을 `nn.Sequential`을 활용하여 생성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dabeb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 사용할 data \n",
    "batch_size = 30\n",
    "num_feature = 100\n",
    "\n",
    "X_train = torch.randn(batch_size, num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc5c673c-068b-4201-8900-3669a84d420a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82fc8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1 코딩 (매 줄마다 주석 필수 )\n",
    "\n",
    "class myFCN(nn.Module):    # nn.Module : neural network모듈을 만들기 위한 베이스 모듈.\n",
    "    def __init__(self):    # 초기화 함수 정의 \n",
    "        super().__init__() # nn.Module을 인스턴스로 불러온 것 = super() / super의 __init__함수를 실행시키는 코드\n",
    "        \n",
    "        # 순차적으로 내가 실행할 모듈들을 정의한다.\n",
    "        # Sequential 함수는 forword함수가 내부적으로 정의되어 있음\n",
    "        self.fc = nn.Sequential(nn.Linear(100, 80),  # nn.Linear(num_feature, 80)\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(80, 50),   # hidden layer\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(50, 1)     # 마지막 layer는 Linear layer로\n",
    "        )\n",
    "        \n",
    "        # 위의 것들을 가지고 forward pass연산을 하는 것을 정의하는 함수\n",
    "        def forward(self, x):\n",
    "            return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "846477b3-37df-43a8-874c-f6c903e797c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance 생성\n",
    "my_model = myFCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a46397a2-bf43-43cd-9c2c-fb1143009f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myFCN(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=80, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=80, out_features=50, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc48cc",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "problem setup에서 구성한 neural network을 `OrderedDict`을 활용하여 생성하세요\n",
    "- 각 layer의 이름을 주고 생성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b7a32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답작성\n",
    "\n",
    "class myFCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # OrderedDict() : 순서를 기억하는 딕셔너리 자료형\n",
    "        self.fc = nn.Sequential(OrderedDict([('lin1', nn.Linear(100, 80)),  # key와 value 만들기 위해 튜플로 작성\n",
    "                      ('relu1', nn.ReLU(True)),\n",
    "                      ('lin2', nn.Linear(80, 50)),\n",
    "                      ('relu2',nn.ReLU(True)),\n",
    "                      ('lin3', nn.Linear(50, 1))\n",
    "                                            ])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Problem1에서 달라진 점은 각 layer에 이름을 준 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "965d76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance 생성\n",
    "my_model = myFCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3659cdb-bbb3-4857-89dc-d8c571140a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myFCN(\n",
       "  (fc): Sequential(\n",
       "    (lin1): Linear(in_features=100, out_features=80, bias=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (lin2): Linear(in_features=80, out_features=50, bias=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
